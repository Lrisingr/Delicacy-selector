Recall(out)
}else{
return(out)
}
}
word_vector<- flatten_Tweet_list(arr[[1]])
word_vector<- as.data.frame(flatten_Tweet_list(arr[[1]]))
View(word_vector)
word_vector_2<- as.data.frame(flatten_Tweet_list(arr[[2]]))
temp2 <- rbind(word_vector,word_vector_2)
View(temp2)
View(id_supply)
View(mat)
View(mat)
length(arr)
# arr <- df$keywords
# where df is dataframe containing result from watson NLU json
# tweet_keyword_vector_1<- as.data.frame(flatten_Tweet_list( arr[[1]] ))
# tweet_keyword_vector_2<- as.data.frame(flatten_Tweet_list( arr[[2]] ))
# create_Keyword_matrix <- rbind(tweet_keyword_vector_1, tweet_keyword_vector_2)
# create_Keyword_matrix <- rbind(create_Keyword_matrix, tweet_keyword_vector_3)
word_matrix<- function(df){
create_Keyword_matrix <- data.frame()
for(i in 1:length(df)){
tweet_keyword_vector <- as.data.frame(flatten_Tweet_list(df[[i]]))
create_Keyword_matrix<- rbind(create_Keyword_matrix,tweet_keyword_vector)
}
return(create_Keyword_matrix)
}
word_matrix(arr)
# arr <- df$keywords
# where df is dataframe containing result from watson NLU json
# tweet_keyword_vector_1<- as.data.frame(flatten_Tweet_list( arr[[1]] ))
# tweet_keyword_vector_2<- as.data.frame(flatten_Tweet_list( arr[[2]] ))
# create_Keyword_matrix <- rbind(tweet_keyword_vector_1, tweet_keyword_vector_2)
# create_Keyword_matrix <- rbind(create_Keyword_matrix, tweet_keyword_vector_3)
word_matrix<- function(){
create_Keyword_matrix <- data.frame()
for(i in 1:length(arr)){
tweet_keyword_vector <- as.data.frame(flatten_Tweet_list(arr[[i]]))
create_Keyword_matrix<- rbind(create_Keyword_matrix,tweet_keyword_vector)
}
return(create_Keyword_matrix)
}
word_matrix(arr)
word_matrix()
word_matrix()
debugSource('F:/R_Projects/Twitter_Trends/word_matrix.R')
word_matrix()
word_matrix()
arr[[14]]
word_matrix<- function(){
create_Keyword_matrix <- data.frame()
for(i in 1:length(arr)){
if( !is.null(arr[[i]])){
tweet_keyword_vector <- as.data.frame(flatten_Tweet_list(arr[[i]]))
create_Keyword_matrix<- rbind(create_Keyword_matrix,tweet_keyword_vector)
}
}
return(create_Keyword_matrix)
}
word_matrix()
debugSource('F:/R_Projects/Twitter_Trends/word_matrix.R', echo=TRUE)
word_matrix()
View(create_Keyword_matrix)
arr[[34]]
arr[[35]]
arr[[36]]
View(mat)
length(arr[[36]])
# arr <- df$keywords
# where df is dataframe containing result from watson NLU json
# tweet_keyword_vector_1<- as.data.frame(flatten_Tweet_list( arr[[1]] ))
# tweet_keyword_vector_2<- as.data.frame(flatten_Tweet_list( arr[[2]] ))
# create_Keyword_matrix <- rbind(tweet_keyword_vector_1, tweet_keyword_vector_2)
# create_Keyword_matrix <- rbind(create_Keyword_matrix, tweet_keyword_vector_3)
word_matrix<- function(){
create_Keyword_matrix <- data.frame()
for(i in 1:length(arr)){
if( !is.null(arr[[i]]) & length(arr[[i]])!=0){
tweet_keyword_vector <- as.data.frame(flatten_Tweet_list(arr[[i]]))
create_Keyword_matrix<- rbind(create_Keyword_matrix,tweet_keyword_vector)
}
}
return(create_Keyword_matrix)
}
word_matrix()
# arr <- df$keywords
# where df is dataframe containing result from watson NLU json
# tweet_keyword_vector_1<- as.data.frame(flatten_Tweet_list( arr[[1]] ))
# tweet_keyword_vector_2<- as.data.frame(flatten_Tweet_list( arr[[2]] ))
# create_Keyword_matrix <- rbind(tweet_keyword_vector_1, tweet_keyword_vector_2)
# create_Keyword_matrix <- rbind(create_Keyword_matrix, tweet_keyword_vector_3)
create_Keyword_matrix <- data.frame()
word_matrix<- function(){
for(i in 1:length(arr)){
if( !is.null(arr[[i]]) & length(arr[[i]])!=0){
tweet_keyword_vector <- as.data.frame(flatten_Tweet_list(arr[[i]]))
create_Keyword_matrix<- rbind(create_Keyword_matrix,tweet_keyword_vector)
}
}
return(create_Keyword_matrix)
}
word_matrix()
# arr <- df$keywords
# where df is dataframe containing result from watson NLU json
# tweet_keyword_vector_1<- as.data.frame(flatten_Tweet_list( arr[[1]] ))
# tweet_keyword_vector_2<- as.data.frame(flatten_Tweet_list( arr[[2]] ))
# create_Keyword_matrix <- rbind(tweet_keyword_vector_1, tweet_keyword_vector_2)
# create_Keyword_matrix <- rbind(create_Keyword_matrix, tweet_keyword_vector_3)
word_matrix<- function(){
create_Keyword_matrix <- data.frame()
for(i in 1:length(arr)){
if( !is.null(arr[[i]]) & length(arr[[i]])!=0){
tweet_keyword_vector <- as.data.frame(flatten_Tweet_list(arr[[i]]))
create_Keyword_matrix<- rbind(create_Keyword_matrix,tweet_keyword_vector)
}
}
return(create_Keyword_matrix)
}
# arr <- df$keywords
# where df is dataframe containing result from watson NLU json
# tweet_keyword_vector_1<- as.data.frame(flatten_Tweet_list( arr[[1]] ))
# tweet_keyword_vector_2<- as.data.frame(flatten_Tweet_list( arr[[2]] ))
# create_Keyword_matrix <- rbind(tweet_keyword_vector_1, tweet_keyword_vector_2)
# create_Keyword_matrix <- rbind(create_Keyword_matrix, tweet_keyword_vector_3)
word_matrix<- function(df){
create_Keyword_matrix <- data.frame()
for(i in 1:length(df)){
if( !is.null(df[[i]]) & length(df[[i]])!=0){
tweet_keyword_vector <- as.data.frame(flatten_Tweet_list(df[[i]]))
create_Keyword_matrix<- rbind(create_Keyword_matrix,tweet_keyword_vector)
}
}
return(create_Keyword_matrix)
}
create_Keyword_matrix <- word_matrix(arr)
View(create_Keyword_matrix)
savehistory("F:/R_Projects/Twitter_Trends/history.txt")
library(rjson)
purch<- fromJSON(paste(readLines("Tweet_Data/hoepfullyworks.json"), collapse=""))
library(tidyjson)
library(jsonlite)
jsontoDF<- jsonlite::fromJSON(txt = "Tweet_Data/data.json")
View(jsontoDF)
View(jsontoDF)
shiny::runApp('Tweet_Data')
# this is not the 'pro' way to store keys,but yeah who cares?
# newbs  http://blog.revolutionanalytics.com/2015/11/how-to-store-and-use-authentication-details-with-r.html
# The watson.keys.load will let you modify this if you like
# ALCHEMY is integrated in to NLU recently
# Conversation for CHATBOTS and stuff
username_conversation <- "3979237a-0f1d-4d69-a729-6504a1879dc7"
password_conversation <- "SqXHy5YjGrG6"
uname_pswd_conversation = paste(username_conversation,":",password_conversation)
#NLU is integrated with Alchemy now
NLU_url <- "https://gateway.watsonplatform.net/natural-language-understanding/api"
username_NLU <- "e9b6f75e-58a9-41a1-88e6-5bfd94b8cdc9"
password_NLU <- "FFqa5d52Jq8u"
uname_pswd_NLU <- paste(username_NLU,":",password_NLU)
#Discovery service
url_disc <- "https://gateway.watsonplatform.net/discovery/api"
username_disc <- "b27e1b7a-82b0-4758-81a2-fb57f986fa53"
password_disc<- "Ev4AsBQUESVn"
uname_pswd_discovery <- paste(username_disc,":",password_disc)
#Twitter API Keys
APIKey = "OMbDvDNJgwLP8XcHvE0CSsdkZ"
APISecret = "SswRDFgryTQnpFZsqWHuTn8dZFt8l0Iqkn8NXA11yOSENbcEOC"
accessToken= "67590362-X5tZo4D5HxEZEhNXwZyAh2aYkkShapfODD7nMmor1"
accessTokenSecret = "rAw9N0igklgBmOpInLNdvrxGByogTHfaHObIzqWagJJN3"
assign(APIKey,value = "APIKey",envir = .GlobalEnv)
assign(APISecret,value = "APISecret",envir = .GlobalEnv)
assign(accessToken,value = "accessToken",envir = .GlobalEnv)
assign(accessTokenSecret,value = "accessTokenSecret",envir = .GlobalEnv)
#add twitter keyword as input parameter from shiny textbox at later stage
library(twitteR)
library(plyr)
twitter_stuff<- function(searchTerm, numTweets){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- twitteR::searchTwitteR(searchString = searchTerm, n=numTweets,lang = "en")
#getUser<- twitteR::getUser("@Deals4Every")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
#UTILITY FUNCTIONS TO CLEAN UP THE DATA
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
## Clean up junk from text
clean.text <- function(tweet_data)
{
tweet_data = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweet_data)
tweet_data = gsub("@\\w+", "", tweet_data)
tweet_data = gsub("[[:punct:]]", "", tweet_data)
tweet_data = gsub("[[:digit:]]", "", tweet_data)
tweet_data = gsub("http\\w+", "", tweet_data)
tweet_data = gsub("[ \t]{2,}", "", tweet_data)
tweet_data = gsub("^\\s+|\\s+$", "", tweet_data)
tweet_data = gsub("amp", "", tweet_data)
tweet_data = sapply(tweet_data, try.tolower)
tweet_data = tweet_data[tweet_data != ""]
names(tweet_data) = NULL
return(tweet_data)
}
# Encryption function
library(digest)
#write encrypted data to a file
write.aes <- function(df,filename, key) {
require(digest)
zz <- textConnection("out","w")
write.csv(df,zz, row.names=F)
close(zz)
out <- paste(out,collapse="\n")
raw <- charToRaw(out)
raw <- c(raw,as.raw(rep(0,16-length(raw)%%16)))
aes <- AES(key,mode="ECB")
aes$encrypt(raw)
writeBin(aes$encrypt(raw),filename)
}
# read encypted data frame from file
read.aes <- function(filename,key) {
require(digest)
dat <- readBin(filename,"raw",n=1000)
aes <- AES(key,mode="ECB")
raw <- aes$decrypt(dat, raw=TRUE)
txt <- rawToChar(raw[raw>0])
read.csv(text=txt, stringsAsFactors = F)
}
key <- as.raw( sample(1:16, 16))
save(key,file = "key.RData")
library(jsonlite)
jsontoDF<- jsonlite::fromJSON(txt = "Tweet_Data/data.json")
# call flatten_tweet_list and create_Keyword_matrix on jsontoDF
#pass tweet_list$keyword data frame and get individual values ,
# before passing this function to  mat<-jsontoDF$keywords, jsontoDF$sentiment and jsontoDF$emotion are lists nested within jsontoDF$keywords list , so to flatenn everything for each keyword in the specific tweet use the below function
flatten_Tweet_list <- function(df){
morelists <- sapply(df, function(xprime) class(xprime)[1]=="list")
out <- c(df[!morelists], unlist(df[morelists], recursive=FALSE))
if(sum(morelists)){
Recall(out)
}else{
return(out)
}
}
# keyword_arr <- jsontoDF$keywords
# where df is dataframe containing result from watson NLU json
# tweet_keyword_vector_1<- as.data.frame(flatten_Tweet_list( keyword_arr[[1]] ))
# tweet_keyword_vector_2<- as.data.frame(flatten_Tweet_list( keyword_arr[[2]] ))
# create_Keyword_matrix <- rbind(tweet_keyword_vector_1, tweet_keyword_vector_2)
# create_Keyword_matrix <- rbind(create_Keyword_matrix, tweet_keyword_vector_3)
# Need to assign a unique ID for each word(string)
#and it's respective tweet ID where that word came from
create_Keyword_matrix<- function(df){
create_Keyword_matrix <- data.frame()
for(i in 1:length(df)){
if( !is.null(df[[i]]) & length(df[[i]])!=0){
tweet_keyword_vector <- as.data.frame(flatten_Tweet_list(df[[i]]))
word_matrix<- rbind(word_matrix,tweet_keyword_vector)
}
}
return(word_matrix)
}
#create_Keyword_matrix(keyword_arr)
library(shiny)
library(twitteR)
library(plyr)
library(httr)
library(RJSONIO)
source("../twitter_stuff.R")
source("../Watson_NLU.R")
# Define UI for application that draws a histogram
ui <- fluidPage(
title = "Tweet Analysis with Watson",
sidebarLayout(
sidebarPanel(
conditionalPanel('input.dataset === "df"',
checkboxGroupInput("show_vars",
"Tweet DB columns",
names(df),
selected = names(df)))),
mainPanel(
textInput("text",
label = h3("What do you want to search about?"),
value = " "),
actionButton(inputId = "action",label = "Go"),
hr(),
fluidRow(column(3, verbatimTextOutput("value"))),
tabsetPanel(id = 'dataset',
tabPanel("table_summary", DT::dataTableOutput("table_summary")),
tabPanel("df",DT::dataTableOutput("table_tweets"))
)
)
)
)
searchTerm <- "#food OR Food"
# Define server logic required to draw a histogram
server <- function(input, output) {
twitt_df <- twitter_stuff(searchTerm,1000)
twFeed_csv<- write.csv(twitt_df,file = "savedTweets.csv",append = TRUE)
df = twitt_df[sample(nrow(twitt_df),500),]
bind_Signal <- data.frame() #take tweet id from tweet dataframe and bind POST content with the tweet_id
bind_Signal_to_df<-data.frame()
#choose columns to display
jsonSignal<-give_me_a_json_damnit(df, bind_Signal, bind_Signal_to_df)
write.table(x = jsonSignal, file= "jsonSignal_TweetID.csv",append = TRUE,row.names = FALSE ,quote = FALSE)
output$table_summary<- DT::renderDataTable({DT::datatable(jsonSignal)})
output$table_tweets<- DT::renderDataTable({DT::datatable(df,
options = list(lengthMenu = c(50, 300, 500)))})
output$value <- renderPrint("Done")
}
# Run the application
shinyApp(ui = ui, server = server)
library(jsonlite)
jsontoDF<- jsonlite::fromJSON(txt = "Tweet_Data/data.json")
# call flatten_tweet_list and create_Keyword_matrix on jsontoDF
#pass tweet_list$keyword data frame and get individual values ,
# before passing this function to  mat<-jsontoDF$keywords, jsontoDF$sentiment and jsontoDF$emotion are lists nested within jsontoDF$keywords list , so to flatenn everything for each keyword in the specific tweet use the below function
flatten_Tweet_list <- function(df){
morelists <- sapply(df, function(xprime) class(xprime)[1]=="list")
out <- c(df[!morelists], unlist(df[morelists], recursive=FALSE))
if(sum(morelists)){
Recall(out)
}else{
return(out)
}
}
# keyword_arr <- jsontoDF$keywords
# where df is dataframe containing result from watson NLU json
# tweet_keyword_vector_1<- as.data.frame(flatten_Tweet_list( keyword_arr[[1]] ))
# tweet_keyword_vector_2<- as.data.frame(flatten_Tweet_list( keyword_arr[[2]] ))
# create_Keyword_matrix <- rbind(tweet_keyword_vector_1, tweet_keyword_vector_2)
# create_Keyword_matrix <- rbind(create_Keyword_matrix, tweet_keyword_vector_3)
# Need to assign a unique ID for each word(string)
#and it's respective tweet ID where that word came from
create_Keyword_matrix<- function(df){
create_Keyword_matrix <- data.frame()
for(i in 1:length(df)){
if( !is.null(df[[i]]) & length(df[[i]])!=0){
tweet_keyword_vector <- as.data.frame(flatten_Tweet_list(df[[i]]))
word_matrix<- rbind(word_matrix,tweet_keyword_vector)
}
}
return(word_matrix)
}
#create_Keyword_matrix(keyword_arr)
create_Keyword_matrix(keyword_arr)
create_Keyword_matrix(keyword_arr)
keyword_arr <- jsontoDF$keywords
create_Keyword_matrix(keyword_arr)
create_Keyword_matrix<- function(df){
word_matrix <- data.frame()
for(i in 1:length(df)){
if( !is.null(df[[i]]) & length(df[[i]])!=0){
tweet_keyword_vector <- as.data.frame(flatten_Tweet_list(df[[i]]))
word_matrix<- rbind(word_matrix,tweet_keyword_vector)
}
}
return(word_matrix)
}
create_Keyword_matrix(keyword_arr)
keywords_matrix<- create_Keyword_matrix(keyword_arr)
View(keywords_matrix)
feed.JSON <-  jsonlite::fromJSON(txt="Tweet_Data/feed.json")
debugSource('F:/R_Projects/Twitter_Trends/Watson_NLU.R', echo=TRUE)
library(shiny)
library(twitteR)
library(plyr)
library(httr)
library(RJSONIO)
source("../twitter_stuff.R")
source("../Watson_NLU.R")
# Define UI for application that draws a histogram
ui <- fluidPage(
title = "Tweet Analysis with Watson",
sidebarLayout(
sidebarPanel(
conditionalPanel('input.dataset === "df"',
checkboxGroupInput("show_vars",
"Tweet DB columns",
names(df),
selected = names(df)))),
mainPanel(
textInput("text",
label = h3("What do you want to search about?"),
value = " "),
actionButton(inputId = "action",label = "Go"),
hr(),
fluidRow(column(3, verbatimTextOutput("value"))),
tabsetPanel(id = 'dataset',
tabPanel("table_summary", DT::dataTableOutput("table_summary")),
tabPanel("df",DT::dataTableOutput("table_tweets"))
)
)
)
)
searchTerm <- "#food OR Food"
# Define server logic required to draw a histogram
server <- function(input, output) {
twitt_df <- twitter_stuff(searchTerm,500)
twFeed_csv<- write.csv(twitt_df,file = "savedTweets.csv",append = TRUE)
df = twitt_df[sample(nrow(twitt_df),500),]
bind_Signal <- data.frame() #take tweet id from tweet dataframe and bind POST content with the tweet_id
bind_Signal_to_df<-data.frame()
#choose columns to display
jsonSignal<-give_me_a_json_damnit(df, bind_Signal, bind_Signal_to_df)
write.table(x = jsonSignal, file= "jsonSignal_TweetID.csv",append = TRUE,row.names = FALSE ,quote = FALSE)
output$table_summary<- DT::renderDataTable({DT::datatable(jsonSignal)})
output$table_tweets<- DT::renderDataTable({DT::datatable(df,
options = list(lengthMenu = c(50, 300, 500)))})
output$value <- renderPrint("Done")
}
# Run the application
shinyApp(ui = ui, server = server)
View(data_frame)
new_SignalContent
Signal
typeof(Signal)
typeof(new_SignalContent)
mad<- []
mad<- vector()
bad_content <- fromJSON(Signal)
library(jsonlite)
bad_content<- jsonlite::fromJSON(Signal)
bad_Signal<- content_type_json(response)
bad_Signal<- content(response, as="parsed", type = fromJSON)
bad_Signal<- content(response, as="parsed")
bad_bad_Signal<- jsonlite::toJSON(bad_Signal,pretty = TRUE)
bad_bad_Signal
typeof(response)
fatsponse<- url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json")
fatsponse<- url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep="")
url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep="")
fatsponse<- fromJSON(POST(url,authenticate(username_NLU,password_NLU)))
fatsponse<- fromJSON(POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),authenticate(username_NLU,password_NLU)))
fatsponse<- fromJSON(POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),authenticate(username_NLU,password_NLU)))
fatsponse<- fromJSON(url,simplifyDataFrame = TRUE)
library(jsonlite)
library(rjson)
library(RJSONIO)
jsontoDF<- jsonlite::fromJSON(txt = "Tweet_Data/data.json")
feed.JSON <-  jsonlite::fromJSON(txt="Tweet_Data/feed.json", flatten = TRUE)
mad.JSON<- lapply(readLines("codebeautify.json"), jsonlite::fromJSON, flatten = TRUE)
rjson_JSON <- rjson::fromJSON( file = "Tweet_Data/feed.json", method = "C", unexpected.escape = "error" )
rjson_JSON <- rjson::fromJSON( file = "Tweet_Data/feed.json", method = "C")
rjson_JSON <- rjson::fromJSON( file = "Tweet_Data/feed.json")
mad.JSON<- lapply(readLines("codebeautify.json"), jsonlite::fromJSON, flatten = TRUE)
