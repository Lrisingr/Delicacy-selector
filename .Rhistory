boo = scan('negative-words.txt',
what='character', comment.char=';')
# Add a few twitter-specific negative phrases
bad_text = c(boo, 'wtf', 'epicfail', 'douchebag')
good_text = c(yay, 'upgrade', ':)', '#iVoted', 'voted')
score.sentiment = function(sentences, good_text, bad_text, .progress='none')
{
require(plyr)
require(stringr)
# we got a vector of sentences. plyr will handle a list
# or a vector as an "l" for us
# we want a simple array of scores back, so we use
# "l" + "a" + "ply" = "laply":
scores = laply(sentences, function(sentence, good_text, bad_text) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('/d+', '', sentence)
#to remove emojis
sentence <- iconv(sentence, 'UTF-8', 'ASCII')
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '/s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, good_text)
neg.matches = match(words, bad_text)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, good_text, bad_text, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
# Call the function and return a data frame
feelthabern <- score.sentiment(feed_sanders, good_text, bad_text, .progress='text')
# Cut the text, just gets in the way
plotdat <- plotdat[c("name", "score")]
# Remove neutral values of 0
plotdat <- plotdat[!plotdat$score == 0, ]
# Nice little quick plot
qplot(factor(score), data=plotdat, geom="bar",
fill=factor(name),
xlab = "Sentiment Score")team
me <- twitteR::getUser("sharavankoushik")
length(tweets_harvey)
tweets <- laply(tweets_harvey,function(t) t$getText())
#UTILITY FUNCTIONS TO CLEAN UP THE DATA
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
## Clean up junk from text
clean.text <- function(tweet_data)
{
tweet_data = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweet_data)
tweet_data = gsub("@\\w+", "", tweet_data)
tweet_data = gsub("[[:punct:]]", "", tweet_data)
tweet_data = gsub("[[:digit:]]", "", tweet_data)
tweet_data = gsub("http\\w+", "", tweet_data)
tweet_data = gsub("[ \t]{2,}", "", tweet_data)
tweet_data = gsub("^\\s+|\\s+$", "", tweet_data)
tweet_data = gsub("amp", "", tweet_data)
tweet_data = sapply(tweet_data, try.tolower)
tweet_data = tweet_data[tweet_data != ""]
names(tweet_data) = NULL
return(tweet_data)
}
#UTILITY FUNCTIONS TO CLEAN UP THE DATA
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
## Clean up junk from text
clean.text <- function(tweet_data)
{
tweet_data = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweet_data)
tweet_data = gsub("@\\w+", "", tweet_data)
tweet_data = gsub("[[:punct:]]", "", tweet_data)
tweet_data = gsub("[[:digit:]]", "", tweet_data)
tweet_data = gsub("http\\w+", "", tweet_data)
tweet_data = gsub("[ \t]{2,}", "", tweet_data)
tweet_data = gsub("^\\s+|\\s+$", "", tweet_data)
tweet_data = gsub("amp", "", tweet_data)
tweet_data = sapply(tweet_data, try.tolower)
tweet_data = tweet_data[tweet_data != ""]
names(tweet_data) = NULL
return(tweet_data)
}
source("Utility_Functions.R")
tweets<- clean.text(tweets)
Encoding(tweets)<-'UTF-8'
Encoding(tweets) <- "UTF=8"
api_feature <- "TextGetCombinedData"
uname_pswd_NLU
output_mode <- "json"
#Performing operations on the Tweet data passing it to the services offered by Watson
text <- URLencode("I like chocolate ice cream and red boots and Molson Beer")
query <- paste(NLU_url,api_feature,"?extract=keyword,entity,concept&apikey=",api_key,"&text=",text,"&outputMode=",output_mode, sep="")
response <- POST(query)
content(response)
response
api_feature <- "TextGetCombinedData"
uname_pswd_NLU
output_mode <- "json"
#Performing operations on the Tweet data passing it to the services offered by Watson
text <- URLencode("I like chocolate ice cream and red boots and Molson Beer")
query <- paste(NLU_url,api_feature,"?extract=keyword,entity,concept&apikey=",uname_pswd_NLU,"&text=",text,"&outputMode=",output_mode, sep="")
response <- POST(query)
content(response)
response
POST(url="https://gateway.watsonplatform.net/natural-language-understanding/api/v1/analyze?version=2017-02-27&url=www.ibm.com&features=keywords,entities",
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json")
)
# Visit https://watson-api-explorer.mybluemix.net/apis/natural-language-understanding-v1#!/Analyze/analyzeGet
# to get the list of parameters
response <- POST(url=paste(url_NLU,"/v1/analyze",
version,"&url=www.ibm.com","&features=keywords,entities","&entities.emotion=true",
"&entities.sentiment=true","&keywords.emotion=true","&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json")
)
response
content(response)
# Visit https://watson-api-explorer.mybluemix.net/apis/natural-language-understanding-v1#!/Analyze/analyzeGet
# to get the list of parameters
response <- POST(url=paste(NLU_url,"/v1/analyze",
version,"&url=www.ibm.com","&features=keywords,entities","&entities.emotion=true",
"&entities.sentiment=true","&keywords.emotion=true","&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json")
)
response
content(response)
url_NLU="https://gateway.watsonplatform.net/natural-language-understanding/api"
response <- POST(url=paste(url_NLU,"/v1/analyze",
version,"&url=www.ibm.com","&features=keywords,entities","&entities.emotion=true",
"&entities.sentiment=true","&keywords.emotion=true","&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json")
)
response
content(response)
response <- POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&url=www.ibm.com",
"&features=keywords,entities",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json")
)
POST(url="https://gateway.watsonplatform.net/natural-language-understanding/api/v1/analyze?version=2017-02-27&url=www.ibm.com&features=keywords,entities",
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json"))
resonse = POST(url="https://gateway.watsonplatform.net/natural-language-understanding/api/v1/analyze?version=2017-02-27&url=www.ibm.com&features=keywords,entities",
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json"))
raw_text <- "IBM is an American multinational technology company headquartered in Armonk, New York, United States, with operations in over 170 countries"
raw_text <- "I love ice cream and unicorns! I love Vancouver"
text <- URLencode(raw_text)
response <- POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",text,
"&features=keywords,entities",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json")
)
install.packages(c("Rcpp", "httr"))
response <- POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",text,
"&features=keywords,entities",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json")
)
#Barking up the Watson services tree
#Authentication and Housekeeping
library("WatsonR")
source("keys.R")
source("twitterSearch.R")
source("Utility_Functions.R")
##sets CERT Global to make a CA Cert go away - http://stackoverflow.com/questions/15347233/ssl-certificate-failed-for-twitter-in-r
options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))
Sys.setlocale(locale="C") # error: input string 1 is invalid in this locale
options(warn=-1) # careful - turns off warnings
watson.keys.display()
me <- twitteR::getUser("sharavankoushik")
length(tweets_harvey)
tweets <- laply(tweets_harvey,function(t) t$getText())
tweets<- clean.text(tweets)
Encoding(tweets) <- "UTF=8"
api_feature <- "TextGetCombinedData"
uname_pswd_NLU
output_mode <- "json"
#Performing operations on the Tweet data passing it to the services offered by Watson
text <- URLencode("I like chocolate ice cream and red boots and Molson Beer")
query <- paste(NLU_url,api_feature,"?extract=keyword,entity,concept&apikey=",uname_pswd_NLU,"&text=",text,"&outputMode=",output_mode, sep="")
response <- POST(query)
content(response)
response
url_NLU="https://gateway.watsonplatform.net/natural-language-understanding/api"
version="?version=2017-02-27"
resonse = POST(url="https://gateway.watsonplatform.net/natural-language-understanding/api/v1/analyze?version=2017-02-27&url=www.ibm.com&features=keywords,entities",
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json"))
raw_text <- "IBM is an American multinational technology company headquartered in Armonk, New York, United States, with operations in over 170 countries"
raw_text <- "I love ice cream and unicorns! I love Vancouver"
text <- URLencode(raw_text)
response <- POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",text,
"&features=keywords,entities",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json")
)
response
response$url
response$status_code
response$cookies
response$headers
content(response)
Signal <- content(response)
Signal$usage
Signal$usage$text_characters
Signal$language
Signal$keywords
Signal$keywords[1]
Signal$entities
Signal
Signal$keywords
key_words <- Signal$keywords
for(i in 1:length(key_words)){
print("hi")
}
for(i in 1:length(key_words)){
print(paste(key_words$text[[i]],key_words$sentiment[[i]]))
}
for(i in 1:length(key_words)){
print(key_words$text[[i]],key_words$sentiment[[i]])
}
for(i in 1:length(key_words)){
print(key_words[[i]]$text,key_words[[i]]$sentiment)
}
for(i in 1:length(key_words)){
print(key_words[[i]]$text)
}
for(i in 1:length(key_words)){
print(paste(key_words[[i]]$text))
}
for(i in 1:length(key_words)){
print(paste(key_words[[i]]$text, key_words[[i]]$sentiment))
}
for(i in 1:length(key_words)){
print(paste(key_words[[i]]$text, key_words[[i]]$sentiment$score))
}
for(i in 1:length(key_words)){
print(paste(key_words[[i]]$text,"Sentiment Score : ", key_words[[i]]$sentiment$score))
}
for(i in 1:length(key_words)){
print(paste(key_words[[i]]$text,"|| Sentiment Score : ", key_words[[i]]$sentiment$score))
}
for(i in 1:length(Signal$entities)){
print("hi")
}
Signal$entities
for(i in 1:length(Signal$entities)){
print(Signal[[i]]$entities)
}
for(i in 1:length(Signal$entities)){
print(Signal$entities[[i]])
}
for(i in 1:length(Signal$entities)){
print(Signal$entities[[i]$type])
}
for(i in 1:length(Signal$entities)){
print(paste(Signal$entities[[i]]$type,Signal$entities[[i]]$text,Signal$entities[[i]]$emotion))
}
for(i in 1:length(Signal$entities)){
print(paste(Signal$entities[[i]]$type,Signal$entities[[i]]$text,Signal$entities[[i]]$disambiguation$subtype))
}
for(i in 1:length(Signal$entities)){
print(paste("Type: ", Signal$entities[[i]]$type,
"Text: ", Signal$entities[[i]]$text,
"SubType : ",Signal$entities[[i]]$disambiguation$subtype))
}
for(i in 1:length(Signal$entities)){
print(paste("Type:", Signal$entities[[i]]$type,
" Text:", Signal$entities[[i]]$text,
" SubType:",Signal$entities[[i]]$disambiguation$subtype))
}
for(i in 1:length(Signal$entities)){
print(paste("Type:",Signal$entities[[i]]$type,
" Text:",Signal$entities[[i]]$text,
" SubType:",Signal$entities[[i]]$disambiguation$subtype))
}
library(twitteR)
library(ROAuth)
source("keys.R")
source("Utility_Functions.R")
source("keys.R")
source("Utility_Functions.R")
source("Twitter_Analysis/server.R")
runApp('Twitter_Analysis')
library(twitteR)
library(ROAuth)
source("keys.R")
source("Utility_Functions.R")
library(shiny)
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
# Define server logic required to draw a histogram
shinyServer(function(input, output) {
#Setup the Twitter Account and pass the tokens
#Lets do the search using console first, later retrieve the search object from the Shiny input text box
tweets_list <- searchTwitter(tolower(input$text), n=2000)
data.frame(tweets_list)
# You can access the value of the widget with input$text, e.g.
output$value <- renderPrint({ input$text })
})
runApp('Twitter_Analysis')
#
# This is the server logic of a Shiny web application. You can run the
# application by clicking 'Run App' above.
#
# Find out more about building applications with Shiny here:
#
#    http://shiny.rstudio.com/
#
#install.packages(c("devtools", "rjson", "bit64", "httr", "twitteR", "ROAuth"))
library(twitteR)
library(ROAuth)
source("keys.R")
source("Utility_Functions.R")
library(shiny)
output
# Define server logic required to draw a histogram
shinyServer(function(input, output) {
# You can access the value of the widget with input$text, e.g.
output$value <- renderPrint({ input$text })
})
#
# This is the server logic of a Shiny web application. You can run the
# application by clicking 'Run App' above.
#
# Find out more about building applications with Shiny here:
#
#    http://shiny.rstudio.com/
#
#install.packages(c("devtools", "rjson", "bit64", "httr", "twitteR", "ROAuth"))
library(twitteR)
library(ROAuth)
source("keys.R")
source("Utility_Functions.R")
library(shiny)
output
# Define server logic required to draw a histogram
server <-shinyServer(function(input, output) {
# You can access the value of the widget with input$text, e.g.
output$value <- renderPrint({ input$text })
})
#
# This is the server logic of a Shiny web application. You can run the
# application by clicking 'Run App' above.
#
# Find out more about building applications with Shiny here:
#
#    http://shiny.rstudio.com/
#
#install.packages(c("devtools", "rjson", "bit64", "httr", "twitteR", "ROAuth"))
library(twitteR)
library(ROAuth)
source("keys.R")
source("Utility_Functions.R")
library(shiny)
# Define server logic required to draw a histogram
server <-shinyServer(function(input, output) {
# You can access the value of the widget with input$text, e.g.
output$value <- renderPrint({ input$text })
})
source("keys.R")
source("Utility_Functions.R")
source("Twitter_Analysis/server.R")
source("Twitter_Analysis/ui.R")
View(server)
runApp('Text_Trends')
runApp('Text_Trends')
library(twitteR)
library(RJSONIO)
library(stringr)
library(tm)
library(plyr)
library(ROAuth)  # if you get this error - need this lib Error: object 'OAuthFactory' not found
library(RCurl)
library(RJSONIO)
library(stringr)
library(tm)
install.packages("tm")
library(tm)
library(plyr)
library(ROAuth)  # if you get this error - need this lib Error: object 'OAuthFactory' not found
library(RCurl)
library(rjson) # JSON for R
library(jsonlite) # JSON parser
library(XML) # XML parser
library(httr) # Tools for URLs and HTTP
library(stringr)
library(data.table) # data shaping
library(reshape2) # data shaping
library(tidyr) # data cleaning
library(dplyr) # data cleaning
library(png) # for the presenting of images
library(plyr)
install.packages("png")
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#Lets do the search using console first, later retrieve the search object from the Shiny input text box
tweets_list <- searchTwitter("#Harvey", n=2000)
tweets_text = laply(tweets_list, function(t) t$getText())
df <- data.frame(tweets_list)
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
View(df)
df$statusSource[1]
df$statusSource[2]
df$statusSource[3]
df$statusSource[4]
df$statusSource[5]
df$statusSource[6]
df$statusSource[7]
#Lets do the search using console first, later retrieve the search object from the Shiny input text box
tweets_list <- searchTwitter("Dosa", n=2000)
#Lets do the search using console first, later retrieve the search object from the Shiny input text box
tweets_list <- searchTwitter("Dosa", n=20000)
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#Lets do the search using console first, later retrieve the search object from the Shiny input text box
#when user enter a term/hashtag/username pass it to the function and give the results on all forms i.e ex: "Dosa" then convert it
tweets_list <- searchTwitter("Dosa OR Dhosa", n=5000)
tweets_text = laply(tweets_list, function(t) t$getText())
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
View(df)
clean.text(tweet_data = tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df$text<- tweets_text
View(df)
tweets_text = laply(tweets_list, function(t) t$getText())
clean.text(tweets_text)
content(tweets_text)
tweets_list <- searchTwitter("Dosa OR Dhosa OR #Dosa OR #Dhosa", n=1000,lang = "en")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
content(tweets_text)
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
View(df)
df["text"] = tweets_clean
View(df)
runApp('tweet_parse')
install.packages("DT")
source("twitterSearch.R")
runApp('Tweet_Data')
runApp('Tweet_Data')
runApp('Tweet_Data')
runApp('Tweet_Data')
runApp('Tweet_Data')
runApp('Tweet_Data')
runApp('Tweet_Data')
runApp('Tweet_Data')
