shiny::runApp('Test')
#add twitter keyword as input parameter from shiny textbox at later stage
twitter_stuff<- function(input){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- searchTwitter("Dosa OR Dhosa", n=1000,lang = "en")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
#add twitter keyword as input parameter from shiny textbox at later stage
twitter_stuff<- function(){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- searchTwitter("Dosa OR Dhosa", n=1000,lang = "en")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
shiny::runApp('Tweet_Data')
# keys.R --
# this is not the 'pro' way to store keys, but it's low barrier to entry for many
# newbs  http://blog.revolutionanalytics.com/2015/11/how-to-store-and-use-authentication-details-with-r.html
# The watson.keys.load will let you modify this if you like
# ALCHEMY is integrated in to NLU recently - AUTHENTICATION / CREDENTIALS  - Platinum
# Conversation for CHATBOTS and stuff
username_conversation <- "3979237a-0f1d-4d69-a729-6504a1879dc7"
password_conversation <- "SqXHy5YjGrG6"
uname_pswd_conversation = paste(username_conversation,":",password_conversation)
#NLU is integrated with Alchemy now
NLU_url <- "https://gateway.watsonplatform.net/natural-language-understanding/api"
username_NLU <- "e9b6f75e-58a9-41a1-88e6-5bfd94b8cdc9"
password_NLU <- "FFqa5d52Jq8u"
uname_pswd_NLU <- paste(username_NLU,":",password_NLU)
#Discovery service
url_disc <- "https://gateway.watsonplatform.net/discovery/api"
username_disc <- "b27e1b7a-82b0-4758-81a2-fb57f986fa53"
password_disc<- "Ev4AsBQUESVn"
uname_pswd_discovery <- paste(username_disc,":",password_disc)
#Twitter API Keys
APIKey = "OMbDvDNJgwLP8XcHvE0CSsdkZ"
APISecret = "SswRDFgryTQnpFZsqWHuTn8dZFt8l0Iqkn8NXA11yOSENbcEOC"
accessToken= "67590362-X5tZo4D5HxEZEhNXwZyAh2aYkkShapfODD7nMmor1"
accessTokenSecret = "rAw9N0igklgBmOpInLNdvrxGByogTHfaHObIzqWagJJN3"
#UTILITY FUNCTIONS TO CLEAN UP THE DATA
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
## Clean up junk from text
clean.text <- function(tweet_data)
{
tweet_data = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweet_data)
tweet_data = gsub("@\\w+", "", tweet_data)
tweet_data = gsub("[[:punct:]]", "", tweet_data)
tweet_data = gsub("[[:digit:]]", "", tweet_data)
tweet_data = gsub("http\\w+", "", tweet_data)
tweet_data = gsub("[ \t]{2,}", "", tweet_data)
tweet_data = gsub("^\\s+|\\s+$", "", tweet_data)
tweet_data = gsub("amp", "", tweet_data)
tweet_data = sapply(tweet_data, try.tolower)
tweet_data = tweet_data[tweet_data != ""]
names(tweet_data) = NULL
return(tweet_data)
}
#add twitter keyword as input parameter from shiny textbox at later stage
twitter_stuff<- function(){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- searchTwitter("Dosa OR Dhosa", n=1000,lang = "en")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
runApp('Tweet_Data')
runApp('Tweet_Data')
runApp('Tweet_Data')
runApp('Tweet_Data')
runApp('Tweet_Data')
#add twitter keyword as input parameter from shiny textbox at later stage
twitter_stuff<- function(){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- searchTwitter("Dosa OR Dhosa", n=1000,lang = "en")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
runApp('Tweet_Data')
runApp('Tweet_Data')
runApp('Tweet_Data')
give_me_a_json_damnit<- function(data_frame,bind_id_with_Signal,bind_id_with_Signal_to_df)
{
url_NLU="https://gateway.watsonplatform.net/natural-language-understanding/api"
version="?version=2017-02-27"
for(i in 1:length(data_frame$text))
{
raw_text<- data_frame$text[i]
text_id<- data_frame$id[i]
encoded_text<- URLencode(raw_text)
response <- POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json"))
#Take the POST json and read the content as text or as default json
Signal <- content(response,as="text")
#Bind each tweet ID with the sentiments/entities/categories produced from Watson
bind_id_with_Signal_to_df<- rbind(bind_id_with_Signal,c(text_id,Signal))
#Just convert the Signal to JSON for future use
new_SignalContent <- fromJSON(Signal)
# key_words <- Signal$keywords  # Used not for atomic vectors i.e remove as="text" in content()
#data_frame <- do.call("rbind", lapply(Signal, as.data.frame))
#x_frame <- rbind(df_new,data_frame)
# for(i in 1:length(key_words)){
#   print(paste(key_words[[i]]$text,"|| Sentiment Score : ", key_words[[i]]$sentiment$score))
# }
# for(i in 1:length(Signal$entities)){
#   print(paste("Type:",Signal$entities[[i]]$type,
#               " Text:",Signal$entities[[i]]$text,
#               " SubType:",Signal$entities[[i]]$disambiguation$subtype))
# }
}
return(bind_id_with_Signal_to_df)
}
#UTILITY FUNCTIONS TO CLEAN UP THE DATA
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
## Clean up junk from text
clean.text <- function(tweet_data)
{
tweet_data = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweet_data)
tweet_data = gsub("@\\w+", "", tweet_data)
tweet_data = gsub("[[:punct:]]", "", tweet_data)
tweet_data = gsub("[[:digit:]]", "", tweet_data)
tweet_data = gsub("http\\w+", "", tweet_data)
tweet_data = gsub("[ \t]{2,}", "", tweet_data)
tweet_data = gsub("^\\s+|\\s+$", "", tweet_data)
tweet_data = gsub("amp", "", tweet_data)
tweet_data = sapply(tweet_data, try.tolower)
tweet_data = tweet_data[tweet_data != ""]
names(tweet_data) = NULL
return(tweet_data)
}
# this is not the 'pro' way to store keys,but yeah who cares?
# newbs  http://blog.revolutionanalytics.com/2015/11/how-to-store-and-use-authentication-details-with-r.html
# The watson.keys.load will let you modify this if you like
# ALCHEMY is integrated in to NLU recently
# Conversation for CHATBOTS and stuff
username_conversation <- "3979237a-0f1d-4d69-a729-6504a1879dc7"
password_conversation <- "SqXHy5YjGrG6"
uname_pswd_conversation = paste(username_conversation,":",password_conversation)
#NLU is integrated with Alchemy now
NLU_url <- "https://gateway.watsonplatform.net/natural-language-understanding/api"
username_NLU <- "e9b6f75e-58a9-41a1-88e6-5bfd94b8cdc9"
password_NLU <- "FFqa5d52Jq8u"
uname_pswd_NLU <- paste(username_NLU,":",password_NLU)
#Discovery service
url_disc <- "https://gateway.watsonplatform.net/discovery/api"
username_disc <- "b27e1b7a-82b0-4758-81a2-fb57f986fa53"
password_disc<- "Ev4AsBQUESVn"
uname_pswd_discovery <- paste(username_disc,":",password_disc)
#Twitter API Keys
APIKey = "OMbDvDNJgwLP8XcHvE0CSsdkZ"
APISecret = "SswRDFgryTQnpFZsqWHuTn8dZFt8l0Iqkn8NXA11yOSENbcEOC"
accessToken= "67590362-X5tZo4D5HxEZEhNXwZyAh2aYkkShapfODD7nMmor1"
accessTokenSecret = "rAw9N0igklgBmOpInLNdvrxGByogTHfaHObIzqWagJJN3"
#add twitter keyword as input parameter from shiny textbox at later stage
twitter_stuff<- function(){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- searchTwitter("Dosa OR Dhosa", n=3000,lang = "en")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
twitter_feed<- twitter_stuff()
twFeed_csv<- write.csv(twitter_feed,file = "savedTweets.csv")
df_sample = twitter_feed[sample(nrow(twitter_feed),500),]
#add twitter keyword as input parameter from shiny textbox at later stage
library(twitteR)
twitter_stuff<- function(){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- searchTwitter("Dosa OR Dhosa", n=3000,lang = "en")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
twitter_feed<- twitter_stuff()
twFeed_csv<- write.csv(twitter_feed,file = "savedTweets.csv")
df_sample = twitter_feed[sample(nrow(twitter_feed),500),]
twFeed_csv<- write.csv(twitter_feed,file = "savedTweets.csv")
#UTILITY FUNCTIONS TO CLEAN UP THE DATA
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
## Clean up junk from text
clean.text <- function(tweet_data)
{
tweet_data = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweet_data)
tweet_data = gsub("@\\w+", "", tweet_data)
tweet_data = gsub("[[:punct:]]", "", tweet_data)
tweet_data = gsub("[[:digit:]]", "", tweet_data)
tweet_data = gsub("http\\w+", "", tweet_data)
tweet_data = gsub("[ \t]{2,}", "", tweet_data)
tweet_data = gsub("^\\s+|\\s+$", "", tweet_data)
tweet_data = gsub("amp", "", tweet_data)
tweet_data = sapply(tweet_data, try.tolower)
tweet_data = tweet_data[tweet_data != ""]
names(tweet_data) = NULL
return(tweet_data)
}
#add twitter keyword as input parameter from shiny textbox at later stage
library(twitteR)
twitter_stuff<- function(){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- searchTwitter("Dosa OR Dhosa", n=3000,lang = "en")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
twitter_feed<- twitter_stuff()
#add twitter keyword as input parameter from shiny textbox at later stage
library(twitteR)
library(plyr)
twitter_stuff<- function(){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- searchTwitter("Dosa OR Dhosa", n=3000,lang = "en")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
twitter_feed<- twitter_stuff()
twFeed_csv<- write.csv(twitter_feed,file = "savedTweets.csv")
df_sample = twitter_feed[sample(nrow(twitter_feed),500),]
shiny::runApp('Tweet_Data')
runApp('Tweet_Data')
runApp('Tweet_Data')
#add twitter keyword as input parameter from shiny textbox at later stage
library(twitteR)
library(plyr)
source("Utility_Functions.R")
twitter_stuff<- function(){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- searchTwitter("Dosa OR Dhosa", n=3000,lang = "en")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
twitter_feed<- twitter_stuff()
twFeed_csv<- write.csv(twitter_feed,file = "savedTweets.csv")
df_sample = twitter_feed[sample(nrow(twitter_feed),500),]
runApp('Tweet_Data')
source("Utility_Functions.R")
library(shiny)
library(twitteR)
library(plyr)
library(httr)
library(RJSONIO)
source("../twitter_stuff.R")
source("../Watson_NLU.R")
source("../Utility_Functions.R")
# Define UI for application that draws a histogram
ui <- fluidPage(
title = "Tweet Analysis with Watson",
sidebarLayout(
sidebarPanel(
conditionalPanel('input.dataset === "df"',
checkboxGroupInput("show_vars",
"Tweet DB columns",
names(df),
selected = names(df)
)
)
),
mainPanel(
textInput("text",
label = h3("What do ypu want to search about?"),
value = "Enter #Tag / @user / term"),
hr(),
fluidRow(
column(3,
verbatimTextOutput("value"))
),
tabsetPanel(
id = 'dataset',
tabPanel("df",DT::dataTableOutput("table_tweets")),
tabPanel("table_summary", DT::dataTableOutput("table_summary"))
)
)
)
)
# Define server logic required to draw a histogram
server <- function(input, output) {
bind_Signal <- data.frame() #take tweet id from tweet dataframe and bind POST content with the tweet_id
bind_Signal_to_df<-data.frame()
#choose columns to display
output$table_tweets <- DT::renderDataTable({
DT::datatable(df_sample[, input$show_vars, drop = FALSE])})
#jsonSignal<-give_me_a_json_damnit(df_sample, bind_Signal, bind_Signal_to_df)
# output$table_summary<- DT::renderDataTable({
#                         DT::datatable(bind_Signal_to_df,
#                                       options = list(lengthMenu = c(50, 300, 500)))})
}
# Run the application
shinyApp(ui = ui, server = server)
runApp('Tweet_Data')
runApp('Tweet_Data')
#UTILITY FUNCTIONS TO CLEAN UP THE DATA
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
## Clean up junk from text
clean.text <- function(tweet_data)
{
tweet_data = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweet_data)
tweet_data = gsub("@\\w+", "", tweet_data)
tweet_data = gsub("[[:punct:]]", "", tweet_data)
tweet_data = gsub("[[:digit:]]", "", tweet_data)
tweet_data = gsub("http\\w+", "", tweet_data)
tweet_data = gsub("[ \t]{2,}", "", tweet_data)
tweet_data = gsub("^\\s+|\\s+$", "", tweet_data)
tweet_data = gsub("amp", "", tweet_data)
tweet_data = sapply(tweet_data, try.tolower)
tweet_data = tweet_data[tweet_data != ""]
names(tweet_data) = NULL
return(tweet_data)
}
runApp('Tweet_Data')
runApp('Tweet_Data')
#Script execution of files and packages
source("Utility_Functions.R")
source("keys.R")
source("twitter_stuff.R")
shiny::runApp('Tweet_Data')
runApp('Tweet_Data')
#add twitter keyword as input parameter from shiny textbox at later stage
library(twitteR)
library(plyr)
source("Utility_Functions.R")
twitter_stuff<- function(){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- searchTwitter("Dosa OR Dhosa", n=3000,lang = "en")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
twitter_feed<- twitter_stuff()
twFeed_csv<- write.csv(twitter_feed,file = "savedTweets.csv")
runApp('Tweet_Data')
runApp('Tweet_Data')
runApp('Tweet_Data')
runApp('Tweet_Data')
runApp('Tweet_Data')
ui <- fluidPage(
title = "Tweet Analysis with Watson",
sidebarLayout(
sidebarPanel(
conditionalPanel('input.dataset === "df"',
checkboxGroupInput("show_vars",
"Tweet DB columns",
names(df),
selected = names(df)
)
)
),
mainPanel(
textInput("text",
label = h3("What do ypu want to search about?"),
value = "Enter #Tag / @user / term"),
hr(),
fluidRow(
column(3,
verbatimTextOutput("value"))
),
tabsetPanel(
id = 'dataset',
tabPanel("df",DT::dataTableOutput("table_tweets")),
tabPanel("table_summary", DT::dataTableOutput("table_summary"))
)
)
)
)
runApp('Tweet_Data')
runApp('Tweet_Data')
give_me_a_json_damnit<- function(data_frame,bind_id_with_Signal,bind_id_with_Signal_to_df)
{
url_NLU="https://gateway.watsonplatform.net/natural-language-understanding/api"
version="?version=2017-02-27"
for(i in 1:length(data_frame$text))
{
raw_text<- data_frame$text[i]
text_id<- data_frame$id[i]
encoded_text<- URLencode(raw_text)
response <- POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json"))
#Take the POST json and read the content as text or as default json
Signal <- content(response,as="text")
#Bind each tweet ID with the sentiments/entities/categories produced from Watson
bind_id_with_Signal_to_df<- rbind(bind_id_with_Signal,c(text_id,Signal))
#Just convert the Signal to JSON for future use
new_SignalContent <- fromJSON(Signal)
# key_words <- Signal$keywords  # Used not for atomic vectors i.e remove as="text" in content()
#data_frame <- do.call("rbind", lapply(Signal, as.data.frame))
#x_frame <- rbind(df_new,data_frame)
# for(i in 1:length(key_words)){
#   print(paste(key_words[[i]]$text,"|| Sentiment Score : ", key_words[[i]]$sentiment$score))
# }
# for(i in 1:length(Signal$entities)){
#   print(paste("Type:",Signal$entities[[i]]$type,
#               " Text:",Signal$entities[[i]]$text,
#               " SubType:",Signal$entities[[i]]$disambiguation$subtype))
# }
}
return(bind_id_with_Signal_to_df)
}
runApp('Tweet_Data')
runApp('Tweet_Data')
