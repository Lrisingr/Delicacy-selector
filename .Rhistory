return(word_matrix_1)
}
word_matrix_1
word_matrix
keywords_matrix<- create_Keyword_matrix(flattended.keywordarr)
heythere<-ldply(keyword_arr,function(s){t(data.frame(unlist(s)))})
View(heythere)
heythere<-ldply(keyword_arr,function(s){data.frame(unlist(s))})
heythere<-rbind.fill(lapply(keyword_arr, function(f) {as.data.frame(Filter(Negate(is.null), f))}))
non.null.list <- lapply(keyword_arr, Filter, f = Negate(is.null))
library(plyr)
rbind.fill(lapply(non.null.list, as.data.frame))
keyword_arr[[2]]
keyword_arr[[1]]
keyword_arr[1]
keyword_arr[1][1]
keyword_arr[1][3]
keyword_arr[1][2]
keyword_arr[1][1]
keyword_arr[[3]
]
keyword_arr[[1]]
unlist(keyword_arr[[1]]$sentiment)
unlist(keyword_arr[[1]]$emotion)
keyword_arr[[8]]
keyword_arr[-keyword_arr[[8]]
]
?checkboxGroupInput()
?checkboxGroupInput(--help)
checkboxGroupInput(--help)
?checkboxGroupInput(--help)
library(httr)
library(shiny)
library(twitteR)
library(plyr)
library(httr)
library(RJSONIO)
source("../twitter_stuff.R")
source("../Watson_NLU.R")
# Define UI for application that draws a histogram
ui <- fluidPage(
title = "Tweet Analysis with Watson",
sidebarLayout(
sidebarPanel(
conditionalPanel('input.dataset === "df"',
checkboxGroupInput("show_vars",
"Tweet selectors",
names(df),
selected = names(df)))),
mainPanel(
textInput("text",
label = h3("What do you want to search about?"),
value = " "),
actionButton(inputId = "action",label = "Go"),
hr(),
fluidRow(column(3, verbatimTextOutput("value"))),
tabsetPanel(id = 'dataset',
tabPanel("table_summary", DT::dataTableOutput("table_summary")),
tabPanel("df",DT::dataTableOutput("table_tweets"))
))))
searchTerm <- "#Dosa OR Dosa"
twitt_df<- data.frame()
# Define server logic required to draw a histogram
server <- function(input, output) {
twitt_df <- twitter_stuff(searchTerm,500)
twFeed_csv<- write.csv(twitt_df,file = "savedTweets.csv",append = TRUE)
df = twitt_df[sample(nrow(twitt_df),500),]
#choose columns to display
jsonSignal<-NLU.results(df)
write.csv(x = jsonSignal, file= "jsonSignal_TweetID.csv",append = TRUE)
output$table_summary<- DT::renderDataTable({DT::datatable(jsonSignal)})
output$table_tweets<- DT::renderDataTable({DT::datatable(df,
options = list(lengthMenu = c(50, 300, 500)))})
output$value <- renderPrint("Done")
}
# Run the application
shinyApp(ui = ui, server = server)
# this is not the 'pro' way to store keys,but yeah who cares?
# newbs  http://blog.revolutionanalytics.com/2015/11/how-to-store-and-use-authentication-details-with-r.html
# The watson.keys.load will let you modify this if you like
# ALCHEMY is integrated in to NLU recently
# Conversation for CHATBOTS and stuff
# username_conversation <- "3979237a-0f1d-4d69-a729-6504a1879dc7"
# password_conversation <- "SqXHy5YjGrG6"
# uname_pswd_conversation = paste(username_conversation,":",password_conversation)
#NLU is integrated with Alchemy now
NLU_url <- "https://gateway.watsonplatform.net/natural-language-understanding/api"
username_NLU <- "e9b6f75e-58a9-41a1-88e6-5bfd94b8cdc9"
password_NLU <- "FFqa5d52Jq8u"
uname_pswd_NLU <- paste(username_NLU,":",password_NLU)
#Discovery service
# url_disc <- "https://gateway.watsonplatform.net/discovery/api"
# username_disc <- "b27e1b7a-82b0-4758-81a2-fb57f986fa53"
# password_disc<- "Ev4AsBQUESVn"
# uname_pswd_discovery <- paste(username_disc,":",password_disc)
#Twitter API Keys
APIKey = "OMbDvDNJgwLP8XcHvE0CSsdkZ"
APISecret = "SswRDFgryTQnpFZsqWHuTn8dZFt8l0Iqkn8NXA11yOSENbcEOC"
accessToken= "67590362-X5tZo4D5HxEZEhNXwZyAh2aYkkShapfODD7nMmor1"
accessTokenSecret = "rAw9N0igklgBmOpInLNdvrxGByogTHfaHObIzqWagJJN3"
# assign(APIKey,value = "APIKey",envir = .GlobalEnv)
# assign(APISecret,value = "APISecret",envir = .GlobalEnv)
# assign(accessToken,value = "accessToken",envir = .GlobalEnv)
# assign(accessTokenSecret,value = "accessTokenSecret",envir = .GlobalEnv)
#
library(httr)
library(jsonlite)
url_NLU="https://gateway.watsonplatform.net/natural-language-understanding/api"
version="?version=2017-02-27"
Bind.tID.Signal<-data.frame()
#take tweet id from tweet dataframe and bind POST content with the tweet_id
bind_id_with_Signal <- data.frame()
NLU.results<- function(data_frame)
{
for(i in 1:length(data_frame$text))
{
raw_text<- data_frame$text[i]
text_id<- data_frame$id[i]
encoded_text<- URLencode(raw_text)
response <- POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=concepts,keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json"))
#Take the POST json and read the content as text or as default json
if(response$status_code == 200){
Signal <- content(response)
Signal.asTxt<- content(response,as = "text")
Signal.JSON <- toJSON(Signal)
write(Signal.asTxt,file="NLU_response.txt",append = TRUE)
#Bind each tweet ID with the sentiments/entities/categories produced from Watson
Signal.with.id= rbind(text_id,Signal)
Bind.tID.Signal<- rbind(bind_id_with_Signal,c(text_id,Signal.JSON))
colnames(Bind.tID.Signal)[1]<- "Tweet_ID"
colnames(Bind.tID.Signal)[2]<- "JsonContent"
#Just convert the Signal to JSON for future use
write(Signal.JSON,file = "NLU_results.json",append = TRUE)
}}
return(Bind.tID.Signal)
}
#add twitter keyword as input parameter from shiny textbox at later stage
library(twitteR)
library(plyr)
twitter_stuff<- function(searchTerm, numTweets){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- twitteR::searchTwitteR(searchString = searchTerm, n=numTweets,lang = "en")
#getUser<- twitteR::getUser("@Deals4Every")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
library(digest)
#UTILITY FUNCTIONS TO CLEAN UP THE DATA
# sets CERT Global to make a CA Cert go away
# http://stackoverflow.com/questions/15347233/ssl-certificate-failed-for-twitter-in-r
housekeeping <- function(){
options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))
Sys.setlocale(locale="C") # error: input string 1 is invalid in this locale
options(warn=-1) # careful - turns off warnings
}
try.tolower <- function(x){
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
## Clean up junk from text
clean.text <- function(tweet_data){
tweet_data = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweet_data)
tweet_data = gsub("@\\w+", "", tweet_data)
tweet_data = gsub("[[:punct:]]", "", tweet_data)
tweet_data = gsub("[[:digit:]]", "", tweet_data)
tweet_data = gsub("http\\w+", "", tweet_data)
tweet_data = gsub("[ \t]{2,}", "", tweet_data)
tweet_data = gsub("^\\s+|\\s+$", "", tweet_data)
tweet_data = gsub("amp", "", tweet_data)
tweet_data = sapply(tweet_data, try.tolower)
tweet_data = tweet_data[tweet_data != ""]
names(tweet_data) = NULL
return(tweet_data)
}
# Encryption function
#write encrypted data to a file
write.aes <- function(df,filename, key) {
require(digest)
zz <- textConnection("out","w")
write.csv(df,zz, row.names=F)
close(zz)
out <- paste(out,collapse="\n")
raw <- charToRaw(out)
raw <- c(raw,as.raw(rep(0,16-length(raw)%%16)))
aes <- AES(key,mode="ECB")
aes$encrypt(raw)
writeBin(aes$encrypt(raw),filename)
}
# read encypted data frame from file
read.aes <- function(filename,key) {
require(digest)
dat <- readBin(filename,"raw",n=1000)
aes <- AES(key,mode="ECB")
raw <- aes$decrypt(dat, raw=TRUE)
txt <- rawToChar(raw[raw>0])
read.csv(text=txt, stringsAsFactors = F)
}
key <- as.raw( sample(1:16, 16))
save(key,file = "key.RData")
library(shiny)
library(twitteR)
library(plyr)
library(httr)
library(RJSONIO)
source("../twitter_stuff.R")
source("../Watson_NLU.R")
# Define UI for application that draws a histogram
ui <- fluidPage(
title = "Tweet Analysis with Watson",
sidebarLayout(
sidebarPanel(
conditionalPanel('input.dataset === "df"',
checkboxGroupInput("show_vars",
"Tweet selectors",
names(df),
selected = names(df)))),
mainPanel(
textInput("text",
label = h3("What do you want to search about?"),
value = " "),
actionButton(inputId = "action",label = "Go"),
hr(),
fluidRow(column(3, verbatimTextOutput("value"))),
tabsetPanel(id = 'dataset',
tabPanel("table_summary", DT::dataTableOutput("table_summary")),
tabPanel("df",DT::dataTableOutput("table_tweets"))
))))
searchTerm <- "#Dosa OR Dosa"
twitt_df<- data.frame()
# Define server logic required to draw a histogram
server <- function(input, output) {
twitt_df <- twitter_stuff(searchTerm,500)
twFeed_csv<- write.csv(twitt_df,file = "savedTweets.csv",append = TRUE)
df = twitt_df[sample(nrow(twitt_df),500),]
#choose columns to display
jsonSignal<-NLU.results(df)
write.csv(x = jsonSignal, file= "jsonSignal_TweetID.csv",append = TRUE)
output$table_summary<- DT::renderDataTable({DT::datatable(jsonSignal)})
output$table_tweets<- DT::renderDataTable({DT::datatable(df,
options = list(lengthMenu = c(50, 300, 500)))})
output$value <- renderPrint("Done")
}
# Run the application
shinyApp(ui = ui, server = server)
library(shiny)
library(twitteR)
library(plyr)
library(httr)
library(RJSONIO)
source("../twitter_stuff.R")
source("../Watson_NLU.R")
library(shiny)
library(twitteR)
library(plyr)
library(httr)
library(RJSONIO)
source("../twitter_stuff.R")
source("../Watson_NLU.R")
# Define UI for application that draws a histogram
ui <- fluidPage(
title = "Tweet Analysis with Watson",
sidebarLayout(
sidebarPanel(
conditionalPanel('input.dataset === "df"',
checkboxGroupInput("show_vars",
"Tweet selectors",
names(df),
selected = names(df)))),
mainPanel(
textInput("text",
label = h3("What do you want to search about?"),
value = " "),
actionButton(inputId = "action",label = "Go"),
hr(),
fluidRow(column(3, verbatimTextOutput("value"))),
tabsetPanel(id = 'dataset',
tabPanel("table_summary", DT::dataTableOutput("table_summary")),
tabPanel("df",DT::dataTableOutput("table_tweets"))
))))
searchTerm <- "#Dosa OR Dosa"
twitt_df<- data.frame()
# Define server logic required to draw a histogram
server <- function(input, output) {
twitt_df <- twitter_stuff(searchTerm,500)
twFeed_csv<- write.csv(twitt_df,file = "savedTweets.csv",append = TRUE)
df = twitt_df[sample(nrow(twitt_df),500),]
#choose columns to display
jsonSignal<-NLU.results(df)
write.csv(x = jsonSignal, file= "jsonSignal_TweetID.csv",append = TRUE)
output$table_summary<- DT::renderDataTable({DT::datatable(jsonSignal)})
output$table_tweets<- DT::renderDataTable({DT::datatable(df,
options = list(lengthMenu = c(50, 300, 500)))})
output$value <- renderPrint("Done")
}
# Run the application
shinyApp(ui = ui, server = server)
library(httr)
library(jsonlite)
url_NLU="https://gateway.watsonplatform.net/natural-language-understanding/api"
version="?version=2017-02-27"
Bind.tID.Signal<-data.frame()
#take tweet id from tweet dataframe and bind POST content with the tweet_id
bind_id_with_Signal <- data.frame()
NLU.results<- function(data_frame)
{
for(i in 1:length(data_frame$text))
{
raw_text<- data_frame$text[i]
text_id<- data_frame$id[i]
encoded_text<- URLencode(raw_text)
response <- POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=concepts,keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json"))
#Take the POST json and read the content as text or as default json
if(response$status_code == 200){
Signal <- content(response)
Signal.asTxt<- content(response,as = "text")
Signal.JSON <- toJSON(Signal)
write(Signal.asTxt,file="NLU_response.txt",append = TRUE)
#Bind each tweet ID with the sentiments/entities/categories produced from Watson
Signal.with.id= rbind(text_id,Signal)
Bind.tID.Signal<- rbind(bind_id_with_Signal,c(text_id,Signal.JSON))
colnames(Bind.tID.Signal)[1]<- "Tweet_ID"
colnames(Bind.tID.Signal)[2]<- "JsonContent"
#Just convert the Signal to JSON for future use
write(Signal.JSON,file = "NLU_results.json",append = TRUE)
}}
return(Bind.tID.Signal)
}
library(httr)
library(jsonlite)
url_NLU="https://gateway.watsonplatform.net/natural-language-understanding/api"
version="?version=2017-02-27"
Bind.tID.Signal<-data.frame()
#take tweet id from tweet dataframe and bind POST content with the tweet_id
bind_id_with_Signal <- data.frame()
NLU.results<- function(data_frame)
{
for(i in 1:length(data_frame$text))
{
raw_text<- data_frame$text[i]
text_id<- data_frame$id[i]
encoded_text<- URLencode(raw_text)
response <- POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=concepts,keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json"))
#Take the POST json and read the content as text or as default json
if(response$status_code == 200){
Signal <- content(response)
Signal.asTxt<- content(response,as = "text")
Signal.JSON <- toJSON(Signal)
write(Signal.asTxt,file="NLU_response.txt",append = TRUE)
#Bind each tweet ID with the sentiments/entities/categories produced from Watson
Signal.with.id= rbind(text_id,Signal)
Bind.tID.Signal<- rbind(bind_id_with_Signal,c(text_id,Signal.JSON))
colnames(Bind.tID.Signal)[1]<- "Tweet_ID"
colnames(Bind.tID.Signal)[2]<- "JsonContent"
#Just convert the Signal to JSON for future use
write(Signal.JSON,file = "NLU_results.json",append = TRUE)
}}
return(Bind.tID.Signal)
}
#add twitter keyword as input parameter from shiny textbox at later stage
library(twitteR)
library(plyr)
twitter_stuff<- function(searchTerm, numTweets){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- twitteR::searchTwitteR(searchString = searchTerm, n=numTweets,lang = "en")
#getUser<- twitteR::getUser("@Deals4Every")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
library(shiny)
library(twitteR)
library(plyr)
library(httr)
library(RJSONIO)
source("../twitter_stuff.R")
source("../Watson_NLU.R")
# Define UI for application that draws a histogram
ui <- fluidPage(
title = "Tweet Analysis with Watson",
sidebarLayout(
sidebarPanel(
conditionalPanel('input.dataset === "df"',
checkboxGroupInput("show_vars",
"Tweet selectors",
names(df),
selected = names(df)))),
mainPanel(
textInput("text",
label = h3("What do you want to search about?"),
value = " "),
actionButton(inputId = "action",label = "Go"),
hr(),
fluidRow(column(3, verbatimTextOutput("value"))),
tabsetPanel(id = 'dataset',
tabPanel("table_summary", DT::dataTableOutput("table_summary")),
tabPanel("df",DT::dataTableOutput("table_tweets"))
))))
searchTerm <- "#Dosa OR Dosa"
twitt_df<- data.frame()
# Define server logic required to draw a histogram
server <- function(input, output) {
twitt_df <- twitter_stuff(searchTerm,500)
twFeed_csv<- write.csv(twitt_df,file = "savedTweets.csv",append = TRUE)
df = twitt_df[sample(nrow(twitt_df),500),]
#choose columns to display
jsonSignal<-NLU.results(df)
write.csv(x = jsonSignal, file= "jsonSignal_TweetID.csv",append = TRUE)
output$table_summary<- DT::renderDataTable({DT::datatable(jsonSignal)})
output$table_tweets<- DT::renderDataTable({DT::datatable(df,
options = list(lengthMenu = c(50, 300, 500)))})
output$value <- renderPrint("Done")
}
# Run the application
shinyApp(ui = ui, server = server)
install.packages("shiny")
library(shiny)
library(twitteR)
library(plyr)
library(httr)
library(RJSONIO)
source("../twitter_stuff.R")
source("../Watson_NLU.R")
# Define UI for application that draws a histogram
ui <- fluidPage(
title = "Tweet Analysis with Watson",
sidebarLayout(
sidebarPanel(
conditionalPanel('input.dataset === "df"',
checkboxGroupInput("show_vars",
"Tweet selectors",
names(df),
selected = names(df)))),
mainPanel(
textInput("text",
label = h3("What do you want to search about?"),
value = " "),
actionButton(inputId = "action",label = "Go"),
hr(),
fluidRow(column(3, verbatimTextOutput("value"))),
tabsetPanel(id = 'dataset',
tabPanel("table_summary", DT::dataTableOutput("table_summary")),
tabPanel("df",DT::dataTableOutput("table_tweets"))
))))
searchTerm <- "#Dosa OR Dosa"
twitt_df<- data.frame()
# Define server logic required to draw a histogram
server <- function(input, output) {
twitt_df <- twitter_stuff(searchTerm,500)
twFeed_csv<- write.csv(twitt_df,file = "savedTweets.csv",append = TRUE)
df = twitt_df[sample(nrow(twitt_df),500),]
#choose columns to display
jsonSignal<-NLU.results(df)
write.csv(x = jsonSignal, file= "jsonSignal_TweetID.csv",append = TRUE)
output$table_summary<- DT::renderDataTable({DT::datatable(jsonSignal)})
output$table_tweets<- DT::renderDataTable({DT::datatable(df,
options = list(lengthMenu = c(50, 300, 500)))})
output$value <- renderPrint("Done")
}
# Run the application
shinyApp(ui = ui, server = server)
