txt <- rawToChar(raw[raw>0])
read.csv(text=txt, stringsAsFactors = F)
}
key <- as.raw( sample(1:16, 16))
save(key,file = "key.RData")
library(jsonlite)
jsontoDF<- jsonlite::fromJSON(txt = "Tweet_Data/data.json")
# call flatten_tweet_list and create_Keyword_matrix on jsontoDF
#pass tweet_list$keyword data frame and get individual values ,
# before passing this function to  mat<-jsontoDF$keywords, jsontoDF$sentiment and jsontoDF$emotion are lists nested within jsontoDF$keywords list , so to flatenn everything for each keyword in the specific tweet use the below function
flatten_Tweet_list <- function(df){
morelists <- sapply(df, function(xprime) class(xprime)[1]=="list")
out <- c(df[!morelists], unlist(df[morelists], recursive=FALSE))
if(sum(morelists)){
Recall(out)
}else{
return(out)
}
}
# keyword_arr <- jsontoDF$keywords
# where df is dataframe containing result from watson NLU json
# tweet_keyword_vector_1<- as.data.frame(flatten_Tweet_list( keyword_arr[[1]] ))
# tweet_keyword_vector_2<- as.data.frame(flatten_Tweet_list( keyword_arr[[2]] ))
# create_Keyword_matrix <- rbind(tweet_keyword_vector_1, tweet_keyword_vector_2)
# create_Keyword_matrix <- rbind(create_Keyword_matrix, tweet_keyword_vector_3)
# Need to assign a unique ID for each word(string)
#and it's respective tweet ID where that word came from
create_Keyword_matrix<- function(df){
create_Keyword_matrix <- data.frame()
for(i in 1:length(df)){
if( !is.null(df[[i]]) & length(df[[i]])!=0){
tweet_keyword_vector <- as.data.frame(flatten_Tweet_list(df[[i]]))
word_matrix<- rbind(word_matrix,tweet_keyword_vector)
}
}
return(word_matrix)
}
#create_Keyword_matrix(keyword_arr)
library(shiny)
library(twitteR)
library(plyr)
library(httr)
library(RJSONIO)
source("../twitter_stuff.R")
source("../Watson_NLU.R")
# Define UI for application that draws a histogram
ui <- fluidPage(
title = "Tweet Analysis with Watson",
sidebarLayout(
sidebarPanel(
conditionalPanel('input.dataset === "df"',
checkboxGroupInput("show_vars",
"Tweet DB columns",
names(df),
selected = names(df)))),
mainPanel(
textInput("text",
label = h3("What do you want to search about?"),
value = " "),
actionButton(inputId = "action",label = "Go"),
hr(),
fluidRow(column(3, verbatimTextOutput("value"))),
tabsetPanel(id = 'dataset',
tabPanel("table_summary", DT::dataTableOutput("table_summary")),
tabPanel("df",DT::dataTableOutput("table_tweets"))
)
)
)
)
searchTerm <- "#food OR Food"
# Define server logic required to draw a histogram
server <- function(input, output) {
twitt_df <- twitter_stuff(searchTerm,1000)
twFeed_csv<- write.csv(twitt_df,file = "savedTweets.csv",append = TRUE)
df = twitt_df[sample(nrow(twitt_df),500),]
bind_Signal <- data.frame() #take tweet id from tweet dataframe and bind POST content with the tweet_id
bind_Signal_to_df<-data.frame()
#choose columns to display
jsonSignal<-give_me_a_json_damnit(df, bind_Signal, bind_Signal_to_df)
write.table(x = jsonSignal, file= "jsonSignal_TweetID.csv",append = TRUE,row.names = FALSE ,quote = FALSE)
output$table_summary<- DT::renderDataTable({DT::datatable(jsonSignal)})
output$table_tweets<- DT::renderDataTable({DT::datatable(df,
options = list(lengthMenu = c(50, 300, 500)))})
output$value <- renderPrint("Done")
}
# Run the application
shinyApp(ui = ui, server = server)
library(jsonlite)
jsontoDF<- jsonlite::fromJSON(txt = "Tweet_Data/data.json")
# call flatten_tweet_list and create_Keyword_matrix on jsontoDF
#pass tweet_list$keyword data frame and get individual values ,
# before passing this function to  mat<-jsontoDF$keywords, jsontoDF$sentiment and jsontoDF$emotion are lists nested within jsontoDF$keywords list , so to flatenn everything for each keyword in the specific tweet use the below function
flatten_Tweet_list <- function(df){
morelists <- sapply(df, function(xprime) class(xprime)[1]=="list")
out <- c(df[!morelists], unlist(df[morelists], recursive=FALSE))
if(sum(morelists)){
Recall(out)
}else{
return(out)
}
}
# keyword_arr <- jsontoDF$keywords
# where df is dataframe containing result from watson NLU json
# tweet_keyword_vector_1<- as.data.frame(flatten_Tweet_list( keyword_arr[[1]] ))
# tweet_keyword_vector_2<- as.data.frame(flatten_Tweet_list( keyword_arr[[2]] ))
# create_Keyword_matrix <- rbind(tweet_keyword_vector_1, tweet_keyword_vector_2)
# create_Keyword_matrix <- rbind(create_Keyword_matrix, tweet_keyword_vector_3)
# Need to assign a unique ID for each word(string)
#and it's respective tweet ID where that word came from
create_Keyword_matrix<- function(df){
create_Keyword_matrix <- data.frame()
for(i in 1:length(df)){
if( !is.null(df[[i]]) & length(df[[i]])!=0){
tweet_keyword_vector <- as.data.frame(flatten_Tweet_list(df[[i]]))
word_matrix<- rbind(word_matrix,tweet_keyword_vector)
}
}
return(word_matrix)
}
#create_Keyword_matrix(keyword_arr)
create_Keyword_matrix(keyword_arr)
create_Keyword_matrix(keyword_arr)
keyword_arr <- jsontoDF$keywords
create_Keyword_matrix(keyword_arr)
create_Keyword_matrix<- function(df){
word_matrix <- data.frame()
for(i in 1:length(df)){
if( !is.null(df[[i]]) & length(df[[i]])!=0){
tweet_keyword_vector <- as.data.frame(flatten_Tweet_list(df[[i]]))
word_matrix<- rbind(word_matrix,tweet_keyword_vector)
}
}
return(word_matrix)
}
create_Keyword_matrix(keyword_arr)
keywords_matrix<- create_Keyword_matrix(keyword_arr)
View(keywords_matrix)
feed.JSON <-  jsonlite::fromJSON(txt="Tweet_Data/feed.json")
debugSource('F:/R_Projects/Twitter_Trends/Watson_NLU.R', echo=TRUE)
library(shiny)
library(twitteR)
library(plyr)
library(httr)
library(RJSONIO)
source("../twitter_stuff.R")
source("../Watson_NLU.R")
# Define UI for application that draws a histogram
ui <- fluidPage(
title = "Tweet Analysis with Watson",
sidebarLayout(
sidebarPanel(
conditionalPanel('input.dataset === "df"',
checkboxGroupInput("show_vars",
"Tweet DB columns",
names(df),
selected = names(df)))),
mainPanel(
textInput("text",
label = h3("What do you want to search about?"),
value = " "),
actionButton(inputId = "action",label = "Go"),
hr(),
fluidRow(column(3, verbatimTextOutput("value"))),
tabsetPanel(id = 'dataset',
tabPanel("table_summary", DT::dataTableOutput("table_summary")),
tabPanel("df",DT::dataTableOutput("table_tweets"))
)
)
)
)
searchTerm <- "#food OR Food"
# Define server logic required to draw a histogram
server <- function(input, output) {
twitt_df <- twitter_stuff(searchTerm,500)
twFeed_csv<- write.csv(twitt_df,file = "savedTweets.csv",append = TRUE)
df = twitt_df[sample(nrow(twitt_df),500),]
bind_Signal <- data.frame() #take tweet id from tweet dataframe and bind POST content with the tweet_id
bind_Signal_to_df<-data.frame()
#choose columns to display
jsonSignal<-give_me_a_json_damnit(df, bind_Signal, bind_Signal_to_df)
write.table(x = jsonSignal, file= "jsonSignal_TweetID.csv",append = TRUE,row.names = FALSE ,quote = FALSE)
output$table_summary<- DT::renderDataTable({DT::datatable(jsonSignal)})
output$table_tweets<- DT::renderDataTable({DT::datatable(df,
options = list(lengthMenu = c(50, 300, 500)))})
output$value <- renderPrint("Done")
}
# Run the application
shinyApp(ui = ui, server = server)
View(data_frame)
new_SignalContent
Signal
typeof(Signal)
typeof(new_SignalContent)
mad<- []
mad<- vector()
bad_content <- fromJSON(Signal)
library(jsonlite)
bad_content<- jsonlite::fromJSON(Signal)
bad_Signal<- content_type_json(response)
bad_Signal<- content(response, as="parsed", type = fromJSON)
bad_Signal<- content(response, as="parsed")
bad_bad_Signal<- jsonlite::toJSON(bad_Signal,pretty = TRUE)
bad_bad_Signal
typeof(response)
fatsponse<- url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json")
fatsponse<- url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep="")
url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep="")
fatsponse<- fromJSON(POST(url,authenticate(username_NLU,password_NLU)))
fatsponse<- fromJSON(POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),authenticate(username_NLU,password_NLU)))
fatsponse<- fromJSON(POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),authenticate(username_NLU,password_NLU)))
fatsponse<- fromJSON(url,simplifyDataFrame = TRUE)
library(jsonlite)
library(rjson)
library(RJSONIO)
jsontoDF<- jsonlite::fromJSON(txt = "Tweet_Data/data.json")
feed.JSON <-  jsonlite::fromJSON(txt="Tweet_Data/feed.json", flatten = TRUE)
mad.JSON<- lapply(readLines("codebeautify.json"), jsonlite::fromJSON, flatten = TRUE)
rjson_JSON <- rjson::fromJSON( file = "Tweet_Data/feed.json", method = "C", unexpected.escape = "error" )
rjson_JSON <- rjson::fromJSON( file = "Tweet_Data/feed.json", method = "C")
rjson_JSON <- rjson::fromJSON( file = "Tweet_Data/feed.json")
mad.JSON<- lapply(readLines("codebeautify.json"), jsonlite::fromJSON, flatten = TRUE)
feed.JSON <-  jsonlite::fromJSON(txt="Tweet_Data/feed.json", flatten = TRUE)
mad.JSON<- lapply(readLines("codebeautify.json"), jsonlite::fromJSON, flatten = TRUE)
shiny::runApp('Tweet_Data')
#add twitter keyword as input parameter from shiny textbox at later stage
library(twitteR)
library(plyr)
twitter_stuff<- function(searchTerm, numTweets){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- twitteR::searchTwitteR(searchString = searchTerm, n=numTweets,lang = "en")
#getUser<- twitteR::getUser("@Deals4Every")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
library(shiny)
library(twitteR)
library(plyr)
library(httr)
library(RJSONIO)
source("../twitter_stuff.R")
source("../Watson_NLU.R")
# Define UI for application that draws a histogram
ui <- fluidPage(
title = "Tweet Analysis with Watson",
sidebarLayout(
sidebarPanel(
conditionalPanel('input.dataset === "df"',
checkboxGroupInput("show_vars",
"Tweet DB columns",
names(df),
selected = names(df)))),
mainPanel(
textInput("text",
label = h3("What do you want to search about?"),
value = " "),
actionButton(inputId = "action",label = "Go"),
hr(),
fluidRow(column(3, verbatimTextOutput("value"))),
tabsetPanel(id = 'dataset',
tabPanel("table_summary", DT::dataTableOutput("table_summary")),
tabPanel("df",DT::dataTableOutput("table_tweets"))
)
)
)
)
searchTerm <- "#food OR Food"
# Define server logic required to draw a histogram
server <- function(input, output) {
twitt_df <- twitter_stuff(searchTerm,500)
twFeed_csv<- write.csv(twitt_df,file = "savedTweets.csv",append = TRUE)
df = twitt_df[sample(nrow(twitt_df),500),]
bind_Signal <- data.frame() #take tweet id from tweet dataframe and bind POST content with the tweet_id
bind_Signal_to_df<-data.frame()
#choose columns to display
jsonSignal<-NLU.results(df, bind_Signal, bind_Signal_to_df)
write.csv(x = jsonSignal, file= "jsonSignal_TweetID.csv",append = TRUE)
output$table_summary<- DT::renderDataTable({DT::datatable(jsonSignal)})
output$table_tweets<- DT::renderDataTable({DT::datatable(df,
options = list(lengthMenu = c(50, 300, 500)))})
output$value <- renderPrint("Done")
}
# Run the application
shinyApp(ui = ui, server = server)
library(httr)
library(jsonlite)
url_NLU="https://gateway.watsonplatform.net/natural-language-understanding/api"
version="?version=2017-02-27"
jsonFile = "jsonFile.json"
NLU.results<- function(data_frame,bind_id_with_Signal,Bind.tID.Signal)
{
for(i in 1:length(data_frame$text))
{
raw_text<- data_frame$text[i]
text_id<- data_frame$id[i]
encoded_text<- URLencode(raw_text)
response <- POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),encode = "json",
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json"))
#Take the POST json and read the content as text or as default json
Signal <- content(response)
Signal.asTxt<- content(response,as = "text")
Signal.JSON <- toJSON(Signal)
write(Signal.asTxt,file="Signal_asTxt.txt",append = TRUE)
#Bind each tweet ID with the sentiments/entities/categories produced from Watson
#bind_id_with_Signal= rbind(text_id,Signal)
Bind.tID.Signal<- rbind(bind_id_with_Signal,c(text_id,Signal.JSON))
colnames(Bind.tID.Signal)[1]<- "Tweet_ID"
colnames(Bind.tID.Signal)[2]<- "JsonContent"
#Just convert the Signal to JSON for future use
write(Signal.JSON,file = "feed_man.json",append = TRUE)
write(',',file = "feed_man.json",append = TRUE)
}
return(Bind.tID.Signal)
}
library(shiny)
library(twitteR)
library(plyr)
library(httr)
library(RJSONIO)
source("../twitter_stuff.R")
source("../Watson_NLU.R")
# Define UI for application that draws a histogram
ui <- fluidPage(
title = "Tweet Analysis with Watson",
sidebarLayout(
sidebarPanel(
conditionalPanel('input.dataset === "df"',
checkboxGroupInput("show_vars",
"Tweet DB columns",
names(df),
selected = names(df)))),
mainPanel(
textInput("text",
label = h3("What do you want to search about?"),
value = " "),
actionButton(inputId = "action",label = "Go"),
hr(),
fluidRow(column(3, verbatimTextOutput("value"))),
tabsetPanel(id = 'dataset',
tabPanel("table_summary", DT::dataTableOutput("table_summary")),
tabPanel("df",DT::dataTableOutput("table_tweets"))
)
)
)
)
searchTerm <- "#food OR Food"
# Define server logic required to draw a histogram
server <- function(input, output) {
twitt_df <- twitter_stuff(searchTerm,500)
twFeed_csv<- write.csv(twitt_df,file = "savedTweets.csv",append = TRUE)
df = twitt_df[sample(nrow(twitt_df),500),]
bind_Signal <- data.frame() #take tweet id from tweet dataframe and bind POST content with the tweet_id
bind_Signal_to_df<-data.frame()
#choose columns to display
jsonSignal<-NLU.results(df, bind_Signal, bind_Signal_to_df)
write.csv(x = jsonSignal, file= "jsonSignal_TweetID.csv",append = TRUE)
output$table_summary<- DT::renderDataTable({DT::datatable(jsonSignal)})
output$table_tweets<- DT::renderDataTable({DT::datatable(df,
options = list(lengthMenu = c(50, 300, 500)))})
output$value <- renderPrint("Done")
}
# Run the application
shinyApp(ui = ui, server = server)
debugSource('F:/R_Projects/Twitter_Trends/Watson_NLU.R', echo=TRUE)
library(shiny)
library(twitteR)
library(plyr)
library(httr)
library(RJSONIO)
source("../twitter_stuff.R")
source("../Watson_NLU.R")
# Define UI for application that draws a histogram
ui <- fluidPage(
title = "Tweet Analysis with Watson",
sidebarLayout(
sidebarPanel(
conditionalPanel('input.dataset === "df"',
checkboxGroupInput("show_vars",
"Tweet DB columns",
names(df),
selected = names(df)))),
mainPanel(
textInput("text",
label = h3("What do you want to search about?"),
value = " "),
actionButton(inputId = "action",label = "Go"),
hr(),
fluidRow(column(3, verbatimTextOutput("value"))),
tabsetPanel(id = 'dataset',
tabPanel("table_summary", DT::dataTableOutput("table_summary")),
tabPanel("df",DT::dataTableOutput("table_tweets"))
)
)
)
)
searchTerm <- "#food OR Food"
# Define server logic required to draw a histogram
server <- function(input, output) {
twitt_df <- twitter_stuff(searchTerm,500)
twFeed_csv<- write.csv(twitt_df,file = "savedTweets.csv",append = TRUE)
df = twitt_df[sample(nrow(twitt_df),500),]
bind_Signal <- data.frame() #take tweet id from tweet dataframe and bind POST content with the tweet_id
bind_Signal_to_df<-data.frame()
#choose columns to display
jsonSignal<-NLU.results(df, bind_Signal, bind_Signal_to_df)
write.csv(x = jsonSignal, file= "jsonSignal_TweetID.csv",append = TRUE)
output$table_summary<- DT::renderDataTable({DT::datatable(jsonSignal)})
output$table_tweets<- DT::renderDataTable({DT::datatable(df,
options = list(lengthMenu = c(50, 300, 500)))})
output$value <- renderPrint("Done")
}
# Run the application
shinyApp(ui = ui, server = server)
jsontoDF<- jsonlite::fromJSON(txt = "Tweet_Data/new_json.json")
jsontoDF<- jsonlite::fromJSON("Tweet_Data/new_json.json")
jsontoDF<- jsonlite::fromJSON("../josn_formatt/new_json.json")
flatten_Tweet_list <- function(df){
morelists <- sapply(df, function(xprime) class(xprime)[1]=="list")
out <- c(df[!morelists], unlist(df[morelists], recursive=FALSE))
if(sum(morelists)){
Recall(out)
}else{
return(out)
}
}
keyword_arr <- jsontoDF$keywords
create_Keyword_matrix<- function(df){
word_matrix <- data.frame()
for(i in 1:length(df)){
if( !is.null(df[[i]]) & length(df[[i]])!=0){
tweet_keyword_vector <- as.data.frame(flatten_Tweet_list(df[[i]]))
word_matrix<- rbind(word_matrix,tweet_keyword_vector)
}
}
return(word_matrix)
}
keywords_matrix<- create_Keyword_matrix(keyword_arr)
View(keywords_matrix)
View(keywords_matrix)
View(keywords_matrix)
View(jsontoDF)
