dat <- readBin(filename,"raw",n=1000)
aes <- AES(key,mode="ECB")
raw <- aes$decrypt(dat, raw=TRUE)
txt <- rawToChar(raw[raw>0])
read.csv(text=txt, stringsAsFactors = F)
}
key <- as.raw( sample(1:16, 16))
save(key,file = "key.RData")
give_me_a_json_damnit<- function(data_frame,bind_id_with_Signal,bind_id_with_Signal_to_df)
{
url_NLU="https://gateway.watsonplatform.net/natural-language-understanding/api"
version="?version=2017-02-27"
jsonFile = "jsonFile.json"
for(i in 1:length(data_frame$text))
{
raw_text<- data_frame$text[i]
text_id<- data_frame$id[i]
encoded_text<- URLencode(raw_text)
response <- POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json"))
#Take the POST json and read the content as text or as default json
Signal <- content(response)
#Bind each tweet ID with the sentiments/entities/categories produced from Watson
#bind_id_with_Signal= cbind(text_id,Signal)
new_SignalContent <- toJSON(Signal)
bind_id_with_Signal_to_df<- rbind(bind_id_with_Signal,c(text_id,new_SignalContent))
#Just convert the Signal to JSON for future use
write(new_SignalContent,file = "feed.json",append = TRUE)
#cat(Signal, file=jsonFile, append=TRUE, sep = "\n")
#key_words <- Signal$keywords  # Used not for atomic vectors i.e remove as="text" in content()
#data_frame <- do.call("rbind", lapply(Signal, as.data.frame))
#x_frame <- rbind(df_new,data_frame)
#  for(i in 1:length(key_words)){
#   print(paste(key_words[[i]]$text,"|| Sentiment Score : ", key_words[[i]]$sentiment$score))
# }
# for(i in 1:length(Signal$entities)){
#   print(paste("Type:",Signal$entities[[i]]$type,
#               " Text:",Signal$entities[[i]]$text,
#               " SubType:",Signal$entities[[i]]$disambiguation$subtype))
# }
}
return(bind_id_with_Signal_to_df)
}
#add twitter keyword as input parameter from shiny textbox at later stage
library(twitteR)
library(plyr)
twitter_stuff<- function(numTweets){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- searchTwitter(searchString = "#Dosa OR Dosa", n=numTweets,lang = "en")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
runApp('Tweet_Data')
View(bind_id_with_Signal_to_df)
give_me_a_json_damnit<- function(data_frame,bind_id_with_Signal,bind_id_with_Signal_to_df)
{
url_NLU="https://gateway.watsonplatform.net/natural-language-understanding/api"
version="?version=2017-02-27"
jsonFile = "jsonFile.json"
for(i in 1:length(data_frame$text))
{
raw_text<- data_frame$text[i]
text_id<- data_frame$id[i]
encoded_text<- URLencode(raw_text)
response <- POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json"))
#Take the POST json and read the content as text or as default json
Signal <- content(response)
#Bind each tweet ID with the sentiments/entities/categories produced from Watson
#bind_id_with_Signal= cbind(text_id,Signal)
new_SignalContent <- toJSON(Signal)
bind_id_with_Signal_to_df<- rbind(bind_id_with_Signal,c(text_id,new_SignalContent))
#Just convert the Signal to JSON for future use
write(new_SignalContent,file = "feed.json",append = TRUE,sep = "")
#cat(Signal, file=jsonFile, append=TRUE, sep = "\n")
#key_words <- Signal$keywords  # Used not for atomic vectors i.e remove as="text" in content()
#data_frame <- do.call("rbind", lapply(Signal, as.data.frame))
#x_frame <- rbind(df_new,data_frame)
#  for(i in 1:length(key_words)){
#   print(paste(key_words[[i]]$text,"|| Sentiment Score : ", key_words[[i]]$sentiment$score))
# }
# for(i in 1:length(Signal$entities)){
#   print(paste("Type:",Signal$entities[[i]]$type,
#               " Text:",Signal$entities[[i]]$text,
#               " SubType:",Signal$entities[[i]]$disambiguation$subtype))
# }
}
return(bind_id_with_Signal_to_df)
}
give_me_a_json_damnit<- function(data_frame,bind_id_with_Signal,bind_id_with_Signal_to_df)
{
url_NLU="https://gateway.watsonplatform.net/natural-language-understanding/api"
version="?version=2017-02-27"
jsonFile = "jsonFile.json"
for(i in 1:length(data_frame$text))
{
raw_text<- data_frame$text[i]
text_id<- data_frame$id[i]
encoded_text<- URLencode(raw_text)
response <- POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json"))
#Take the POST json and read the content as text or as default json
Signal <- content(response)
#Bind each tweet ID with the sentiments/entities/categories produced from Watson
#bind_id_with_Signal= cbind(text_id,Signal)
new_SignalContent <- toJSON(Signal)
bind_id_with_Signal_to_df<- cbind(bind_id_with_Signal,c(text_id,new_SignalContent))
colnames(bind_id_with_Signal_to_df)[1]<- "Tweet_ID"
colnames(bind_id_with_Signal_to_df)[2]<- "JsonContent"
#Just convert the Signal to JSON for future use
write(new_SignalContent,file = "feed_col.json",append = TRUE,sep = "")
#cat(Signal, file=jsonFile, append=TRUE, sep = "\n")
#key_words <- Signal$keywords  # Used not for atomic vectors i.e remove as="text" in content()
#data_frame <- do.call("rbind", lapply(Signal, as.data.frame))
#x_frame <- rbind(df_new,data_frame)
#  for(i in 1:length(key_words)){
#   print(paste(key_words[[i]]$text,"|| Sentiment Score : ", key_words[[i]]$sentiment$score))
# }
# for(i in 1:length(Signal$entities)){
#   print(paste("Type:",Signal$entities[[i]]$type,
#               " Text:",Signal$entities[[i]]$text,
#               " SubType:",Signal$entities[[i]]$disambiguation$subtype))
# }
}
return(bind_id_with_Signal_to_df)
}
give_me_a_json_damnit<- function(data_frame,bind_id_with_Signal,bind_id_with_Signal_to_df)
{
url_NLU="https://gateway.watsonplatform.net/natural-language-understanding/api"
version="?version=2017-02-27"
jsonFile = "jsonFile.json"
for(i in 1:length(data_frame$text))
{
raw_text<- data_frame$text[i]
text_id<- data_frame$id[i]
encoded_text<- URLencode(raw_text)
response <- POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json"))
#Take the POST json and read the content as text or as default json
Signal <- content(response)
#Bind each tweet ID with the sentiments/entities/categories produced from Watson
#bind_id_with_Signal= cbind(text_id,Signal)
new_SignalContent <- toJSON(Signal)
bind_id_with_Signal_to_df<- cbind(bind_id_with_Signal,c(text_id,new_SignalContent))
colnames(bind_id_with_Signal_to_df)[1]<- "Tweet_ID"
colnames(bind_id_with_Signal_to_df)[2]<- "JsonContent"
#Just convert the Signal to JSON for future use
write(new_SignalContent,file = "feed_col.json",append = TRUE,sep = "")
#cat(Signal, file=jsonFile, append=TRUE, sep = "\n")
#key_words <- Signal$keywords  # Used not for atomic vectors i.e remove as="text" in content()
#data_frame <- do.call("rbind", lapply(Signal, as.data.frame))
#x_frame <- rbind(df_new,data_frame)
#  for(i in 1:length(key_words)){
#   print(paste(key_words[[i]]$text,"|| Sentiment Score : ", key_words[[i]]$sentiment$score))
# }
# for(i in 1:length(Signal$entities)){
#   print(paste("Type:",Signal$entities[[i]]$type,
#               " Text:",Signal$entities[[i]]$text,
#               " SubType:",Signal$entities[[i]]$disambiguation$subtype))
# }
}
return(bind_id_with_Signal_to_df)
}
runApp('Tweet_Data')
give_me_a_json_damnit<- function(data_frame,bind_id_with_Signal,bind_id_with_Signal_to_df)
{
url_NLU="https://gateway.watsonplatform.net/natural-language-understanding/api"
version="?version=2017-02-27"
jsonFile = "jsonFile.json"
for(i in 1:length(data_frame$text))
{
raw_text<- data_frame$text[i]
text_id<- data_frame$id[i]
encoded_text<- URLencode(raw_text)
response <- POST(url=paste(
url_NLU,
"/v1/analyze",
version,
"&text=",encoded_text,
"&features=keywords,entities,categories",
"&entities.emotion=true",
"&entities.sentiment=true",
"&keywords.emotion=true",
"&keywords.sentiment=true",
sep=""),
authenticate(username_NLU,password_NLU),
add_headers("Content-Type"="application/json"))
#Take the POST json and read the content as text or as default json
Signal <- content(response)
#Bind each tweet ID with the sentiments/entities/categories produced from Watson
#bind_id_with_Signal= cbind(text_id,Signal)
new_SignalContent <- toJSON(Signal)
bind_id_with_Signal_to_df<- rbind(bind_id_with_Signal,c(text_id,new_SignalContent))
colnames(bind_id_with_Signal_to_df)[1]<- "Tweet_ID"
colnames(bind_id_with_Signal_to_df)[2]<- "JsonContent"
#Just convert the Signal to JSON for future use
write(new_SignalContent,file = "feed_col.json",append = TRUE,sep = "")
#cat(Signal, file=jsonFile, append=TRUE, sep = "\n")
#key_words <- Signal$keywords  # Used not for atomic vectors i.e remove as="text" in content()
#data_frame <- do.call("rbind", lapply(Signal, as.data.frame))
#x_frame <- rbind(df_new,data_frame)
#  for(i in 1:length(key_words)){
#   print(paste(key_words[[i]]$text,"|| Sentiment Score : ", key_words[[i]]$sentiment$score))
# }
# for(i in 1:length(Signal$entities)){
#   print(paste("Type:",Signal$entities[[i]]$type,
#               " Text:",Signal$entities[[i]]$text,
#               " SubType:",Signal$entities[[i]]$disambiguation$subtype))
# }
}
return(bind_id_with_Signal_to_df)
}
runApp('Tweet_Data')
purch<- cat(file = "Tweet_Data/CopyOffeed_col.json")
purch<- source("Tweet_Data/CopyOffeed_col.json")
purch<- fromJSON(file="Tweet_Data/CopyOffeed_col.json")
library(RJSONIO)
library(jsonlite)
purch<- fromJSON(file="Tweet_Data/CopyOffeed_col.json")
library(RJSONIO)
library(jsonlite)
purch<- fromJSON(txt="Tweet_Data/CopyOffeed_col.json")
library(RJSONIO)
library(jsonlite)
purch<- fromJSON("Tweet_Data/CopyOffeed_col.json",flatten=TRUE)
library(RJSONIO)
library(jsonlite)
purch<- fromJSON("Tweet_Data/feed.json",flatten=TRUE)
library(rjson)
purch<- fromJSON("Tweet_Data/feed.json")
library(rjson)
purch<- fromJSON(file = "Tweet_Data/feed.json")
purch$categories
library(rjson)
purch<- fromJSON(paste(readLines("Tweet_Data/feed.json"), collapse=""))
library(rjson)
purch<- fromJSON(paste(readLines("Tweet_Data/feed.json"), collapse=""))
library(rjson)
purch<- fromJSON(paste(readLines("Tweet_Data/feed_col.json"), collapse=""))
install.packages("tidyjson")
library(rjson)
purch<- fromJSON(paste(readLines("Tweet_Data/feed_col.json"), collapse=""))
library(tidyjson)
cab<-tidyjson::read_json("Tweet_Data/CopyOffeed_col.json")
shiny::runApp('Tweet_Data')
View(twitt_df)
mat<- jsonlite::fromJSON(txt = "Tweet_Data/data.json")
mat.i<- lapply(mat,function(x){unlist(x)})
mat.df<- do.call("cbind",mat.i)
mat.df<- as.data.frame(mat.df)
View(mat)
runApp('Tweet_Data')
View(twitt_df)
#add twitter keyword as input parameter from shiny textbox at later stage
library(twitteR)
library(plyr)
twitter_stuff<- function(numTweets){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- searchTwitter(searchString = "@Deals4Every", n=numTweets,lang = "en")
#getUser<- twitteR::getUser("@Deals4Every")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
library(shiny)
library(twitteR)
library(plyr)
library(httr)
library(RJSONIO)
source("../twitter_stuff.R")
source("../Watson_NLU.R")
# Define UI for application that draws a histogram
ui <- fluidPage(
title = "Tweet Analysis with Watson",
sidebarLayout(
sidebarPanel(
conditionalPanel('input.dataset === "df"',
checkboxGroupInput("show_vars",
"Tweet DB columns",
names(df),
selected = names(df)))),
mainPanel(
textInput("text",
label = h3("What do you want to search about?"),
value = " "),
actionButton(inputId = "action",label = "Go"),
hr(),
fluidRow(column(3, verbatimTextOutput("value"))),
tabsetPanel(id = 'dataset',
tabPanel("table_summary", DT::dataTableOutput("table_summary")),
tabPanel("df",DT::dataTableOutput("table_tweets"))
)
)
)
)
# Define server logic required to draw a histogram
server <- function(input, output) {
twitt_df <- twitter_stuff(1000)
twFeed_csv<- write.csv(twitt_df,file = "savedTweets.csv",append = TRUE)
df = twitt_df[sample(nrow(twitt_df),500),]
bind_Signal <- data.frame() #take tweet id from tweet dataframe and bind POST content with the tweet_id
bind_Signal_to_df<-data.frame()
#choose columns to display
jsonSignal<-give_me_a_json_damnit(df, bind_Signal, bind_Signal_to_df)
output$table_summary<- DT::renderDataTable({DT::datatable(jsonSignal)})
output$table_tweets<- DT::renderDataTable({DT::datatable(df,options = list(lengthMenu = c(50, 300, 500)))})
output$value <- renderPrint("Done")
}
# Run the application
shinyApp(ui = ui, server = server)
runApp('Tweet_Data')
View(twitt_df)
#add twitter keyword as input parameter from shiny textbox at later stage
library(twitteR)
library(plyr)
twitter_stuff<- function(numTweets){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- twitteR::getUser(user = "@Deals4Every", n=numTweets,lang = "en")
#getUser<- twitteR::getUser("@Deals4Every")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
library(shiny)
library(twitteR)
library(plyr)
library(httr)
library(RJSONIO)
source("../twitter_stuff.R")
source("../Watson_NLU.R")
# Define UI for application that draws a histogram
ui <- fluidPage(
title = "Tweet Analysis with Watson",
sidebarLayout(
sidebarPanel(
conditionalPanel('input.dataset === "df"',
checkboxGroupInput("show_vars",
"Tweet DB columns",
names(df),
selected = names(df)))),
mainPanel(
textInput("text",
label = h3("What do you want to search about?"),
value = " "),
actionButton(inputId = "action",label = "Go"),
hr(),
fluidRow(column(3, verbatimTextOutput("value"))),
tabsetPanel(id = 'dataset',
tabPanel("table_summary", DT::dataTableOutput("table_summary")),
tabPanel("df",DT::dataTableOutput("table_tweets"))
)
)
)
)
# Define server logic required to draw a histogram
server <- function(input, output) {
twitt_df <- twitter_stuff(1000)
twFeed_csv<- write.csv(twitt_df,file = "savedTweets.csv",append = TRUE)
df = twitt_df[sample(nrow(twitt_df),500),]
bind_Signal <- data.frame() #take tweet id from tweet dataframe and bind POST content with the tweet_id
bind_Signal_to_df<-data.frame()
#choose columns to display
jsonSignal<-give_me_a_json_damnit(df, bind_Signal, bind_Signal_to_df)
output$table_summary<- DT::renderDataTable({DT::datatable(jsonSignal)})
output$table_tweets<- DT::renderDataTable({DT::datatable(df,options = list(lengthMenu = c(50, 300, 500)))})
output$value <- renderPrint("Done")
}
# Run the application
shinyApp(ui = ui, server = server)
#add twitter keyword as input parameter from shiny textbox at later stage
library(twitteR)
library(plyr)
twitter_stuff<- function(numTweets){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- twitteR::getUser(user = "@Deals4Every")
#getUser<- twitteR::getUser("@Deals4Every")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
library(shiny)
library(twitteR)
library(plyr)
library(httr)
library(RJSONIO)
source("../twitter_stuff.R")
source("../Watson_NLU.R")
# Define UI for application that draws a histogram
ui <- fluidPage(
title = "Tweet Analysis with Watson",
sidebarLayout(
sidebarPanel(
conditionalPanel('input.dataset === "df"',
checkboxGroupInput("show_vars",
"Tweet DB columns",
names(df),
selected = names(df)))),
mainPanel(
textInput("text",
label = h3("What do you want to search about?"),
value = " "),
actionButton(inputId = "action",label = "Go"),
hr(),
fluidRow(column(3, verbatimTextOutput("value"))),
tabsetPanel(id = 'dataset',
tabPanel("table_summary", DT::dataTableOutput("table_summary")),
tabPanel("df",DT::dataTableOutput("table_tweets"))
)
)
)
)
# Define server logic required to draw a histogram
server <- function(input, output) {
twitt_df <- twitter_stuff(1000)
twFeed_csv<- write.csv(twitt_df,file = "savedTweets.csv",append = TRUE)
df = twitt_df[sample(nrow(twitt_df),500),]
bind_Signal <- data.frame() #take tweet id from tweet dataframe and bind POST content with the tweet_id
bind_Signal_to_df<-data.frame()
#choose columns to display
jsonSignal<-give_me_a_json_damnit(df, bind_Signal, bind_Signal_to_df)
output$table_summary<- DT::renderDataTable({DT::datatable(jsonSignal)})
output$table_tweets<- DT::renderDataTable({DT::datatable(df,options = list(lengthMenu = c(50, 300, 500)))})
output$value <- renderPrint("Done")
}
# Run the application
shinyApp(ui = ui, server = server)
#add twitter keyword as input parameter from shiny textbox at later stage
library(twitteR)
library(plyr)
twitter_stuff<- function(searchTerm, numTweets){
#Setup the Twitter Account and pass the tokens
setup_twitter_oauth(APIKey,APISecret,accessToken,accessTokenSecret)
#input text "Dosa OR Dhosa OR #Dosa OR #Dhosa
tweets_list <- twitteR::searchTwitteR(searchString = searchTerm, n=numTweets,lang = "en")
#getUser<- twitteR::getUser("@Deals4Every")
tweets_text = laply(tweets_list, function(t) t$getText())
tweets_clean <- clean.text(tweets_text)
#convert the tweets_text object to a data frame
#make data frame
df <- do.call("rbind", lapply(tweets_list, as.data.frame))
df["text"] = tweets_clean
return(df)
}
